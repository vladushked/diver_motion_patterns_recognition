{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \" + DEVICE)\n",
    "if torch.backends.cudnn.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_with_norm(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, header=None))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, header=None))\n",
    "    \n",
    "    # Shift labels in PyTorch classification models labels start from 0\n",
    "    y_data = y_data - 1\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, sep=\"\\t\", header=None, dtype=np.float32))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, sep=\"\\t\", header=None, dtype=np.int_))\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 16\n",
    "\n",
    "X_all, y_all = read_data(\"final_poses_withot_D_from_videos.csv\",\"final_labels_withot_D_from_videos.csv\", SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2689"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "val_size = int(y_all.shape[0] * 0.2)\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = X_all[-val_size:], y_all[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_all[-val_size:], y_all[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                               torch.tensor(y_train, dtype=torch.long).squeeze())\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                             torch.tensor(y_val, dtype=torch.long).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_classes, lstm_hidden_dim=256, fc_hidden_dim=256, n_lstm_layers=2):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        \n",
    "        self._lstm = nn.LSTM(input_size=input_dim,\n",
    "                             hidden_size=lstm_hidden_dim,\n",
    "                             num_layers=n_lstm_layers,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        self._fc = nn.Sequential(nn.Linear(lstm_hidden_dim, fc_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc_hidden_dim, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self._lstm.forward(x)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        fc_output = self._fc.forward(lstm_output)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, criterion, batches, phase='train'):\n",
    "    is_train = phase == 'train'\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_predictions = 0\n",
    "    \n",
    "    correct_predictions = 0\n",
    "\n",
    "    for X_batch, y_batch in batches:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = criterion.forward(y_pred, y_batch)\n",
    "    \n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * y_batch.shape[0]\n",
    "        correct_predictions += (torch.argmax(y_pred, dim=1) == y_batch).sum().item()\n",
    "        n_predictions += y_batch.shape[0]\n",
    "\n",
    "    epoch_loss = epoch_loss / n_predictions\n",
    "    epoch_accuracy = correct_predictions / n_predictions\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, n_epoch, batch_size, train_dataset, val_dataset, backup_name):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    train_batches = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    val_batches = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_accuracy = run_epoch(model, optimizer, criterion, train_batches, phase='train')\n",
    "        val_loss, val_accuracy = run_epoch(model, optimizer, criterion, val_batches, phase='val')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), backup_name)\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        print(\"Train loss: \" + str(train_loss) + \", accuracy: \" + str(train_accuracy))\n",
    "        print(\"Val loss: \" + str(val_loss) + \", accuracy: \" + str(val_accuracy) + \"\\n\\n\")\n",
    "        \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "source": [
    "N_CLASSES = 15\n",
    "INPUT_DIM = 36"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "Train loss: 2.5475490634084057, accuracy: 0.2026775753068055\n",
      "Val loss: 2.3049066645720493, accuracy: 0.27891409445890664\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 2.0784351727739945, accuracy: 0.31238378579397547\n",
      "Val loss: 1.8235280966838525, accuracy: 0.39085161770174787\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 1.769719611849205, accuracy: 0.370397917441428\n",
      "Val loss: 1.6978346623200473, accuracy: 0.39791744142804014\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 1.609328029544968, accuracy: 0.4187430271476385\n",
      "Val loss: 1.5720235032634515, accuracy: 0.4369654146522871\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 1.543640068713153, accuracy: 0.42618073633320935\n",
      "Val loss: 1.4766992452908023, accuracy: 0.4696913350687988\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 1.4766899859785545, accuracy: 0.4574191149126069\n",
      "Val loss: 1.3395355590084044, accuracy: 0.5113425065079955\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 1.384676710031608, accuracy: 0.5039047973224247\n",
      "Val loss: 1.3723277179846403, accuracy: 0.49014503532911863\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 1.34262731187004, accuracy: 0.523986612123466\n",
      "Val loss: 1.1941352069666058, accuracy: 0.5738192636667906\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 1.2118727995942364, accuracy: 0.5656377835626627\n",
      "Val loss: 1.2032534022808252, accuracy: 0.5838601710673113\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 1.2028979649549083, accuracy: 0.5734473782075121\n",
      "Val loss: 1.1740639662822412, accuracy: 0.5760505764224619\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 1.1282921889848805, accuracy: 0.5961323912235031\n",
      "Val loss: 1.0772436972487234, accuracy: 0.6355522499070286\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Train loss: 1.1230184295089682, accuracy: 0.6121234659724805\n",
      "Val loss: 1.01723581975878, accuracy: 0.6284864261807364\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Train loss: 1.0164452210905472, accuracy: 0.6507995537374489\n",
      "Val loss: 1.0338206198544428, accuracy: 0.602826329490517\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Train loss: 1.0698370933311054, accuracy: 0.612495351431759\n",
      "Val loss: 1.1051030071218824, accuracy: 0.5905541093343251\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Train loss: 1.0028520067547635, accuracy: 0.6396429899590926\n",
      "Val loss: 0.9136324910347947, accuracy: 0.6838973596132392\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Train loss: 1.0143555294585165, accuracy: 0.6437337300111565\n",
      "Val loss: 1.1478215445812505, accuracy: 0.5619189289698773\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Train loss: 0.9880462660360354, accuracy: 0.6481963555224991\n",
      "Val loss: 0.9288133655440279, accuracy: 0.6507995537374489\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Train loss: 0.9331683500764449, accuracy: 0.6809222759390108\n",
      "Val loss: 0.9966861669260935, accuracy: 0.6507995537374489\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Train loss: 0.9485494461632342, accuracy: 0.6705094830792117\n",
      "Val loss: 0.8647939216341393, accuracy: 0.6980290070658237\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Train loss: 0.8469178006302339, accuracy: 0.7076980290070658\n",
      "Val loss: 0.8386954297005088, accuracy: 0.6991446634436593\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Train loss: 0.9137882608248425, accuracy: 0.6760877649683897\n",
      "Val loss: 0.878749091858401, accuracy: 0.687988099665303\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "Train loss: 0.8341703537273868, accuracy: 0.7129044254369654\n",
      "Val loss: 0.8970961866878852, accuracy: 0.6723689103756043\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "Train loss: 0.9390113525294868, accuracy: 0.6693938267013759\n",
      "Val loss: 0.8521829601910621, accuracy: 0.6987727779843809\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "Train loss: 0.8437082525432753, accuracy: 0.7088136853849014\n",
      "Val loss: 0.7785365855263795, accuracy: 0.7385645221271848\n",
      "\n",
      "\n",
      "Epoch: 24\n",
      "Train loss: 0.8478689237614465, accuracy: 0.7010040907400521\n",
      "Val loss: 0.7567763167183033, accuracy: 0.7378207512086278\n",
      "\n",
      "\n",
      "Epoch: 25\n",
      "Train loss: 0.8953424286292627, accuracy: 0.678319077724061\n",
      "Val loss: 0.7485695902234606, accuracy: 0.7348456675343994\n",
      "\n",
      "\n",
      "Epoch: 26\n",
      "Train loss: 0.788359127464947, accuracy: 0.7155076236519152\n",
      "Val loss: 0.8530128964397179, accuracy: 0.687244328746746\n",
      "\n",
      "\n",
      "Epoch: 27\n",
      "Train loss: 0.8022378433943238, accuracy: 0.7095574563034586\n",
      "Val loss: 0.7833080519393901, accuracy: 0.7229453328374861\n",
      "\n",
      "\n",
      "Epoch: 28\n",
      "Train loss: 0.7619840894857274, accuracy: 0.7314986984008925\n",
      "Val loss: 0.6757838788552105, accuracy: 0.7701747861658609\n",
      "\n",
      "\n",
      "Epoch: 29\n",
      "Train loss: 0.7036853573631735, accuracy: 0.7493492004462625\n",
      "Val loss: 0.8180379132525811, accuracy: 0.7173670509483079\n",
      "\n",
      "\n",
      "Epoch: 30\n",
      "Train loss: 0.8408173642756106, accuracy: 0.7017478616586091\n",
      "Val loss: 0.6704310613212288, accuracy: 0.761249535143176\n",
      "\n",
      "\n",
      "Epoch: 31\n",
      "Train loss: 0.7443200113738767, accuracy: 0.7415396058014132\n",
      "Val loss: 0.6552367725085042, accuracy: 0.7731498698400893\n",
      "\n",
      "\n",
      "Epoch: 32\n",
      "Train loss: 0.704692486627876, accuracy: 0.7500929713648197\n",
      "Val loss: 0.6838663376676588, accuracy: 0.7616214206024544\n",
      "\n",
      "\n",
      "Epoch: 33\n",
      "Train loss: 0.665764901951903, accuracy: 0.766084046113797\n",
      "Val loss: 0.7756223816018691, accuracy: 0.7032354034957233\n",
      "\n",
      "\n",
      "Epoch: 34\n",
      "Train loss: 0.7786017818582152, accuracy: 0.7147638527333581\n",
      "Val loss: 0.7294749659913852, accuracy: 0.7456303458534771\n",
      "\n",
      "\n",
      "Epoch: 35\n",
      "Train loss: 0.6861973544974095, accuracy: 0.7623651915210116\n",
      "Val loss: 0.6381594214176946, accuracy: 0.7779843808107103\n",
      "\n",
      "\n",
      "Epoch: 36\n",
      "Train loss: 0.6160871625146089, accuracy: 0.7835626626998884\n",
      "Val loss: 0.5992389828044271, accuracy: 0.7794719226478245\n",
      "\n",
      "\n",
      "Epoch: 37\n",
      "Train loss: 0.5768301591371329, accuracy: 0.7939754555596876\n",
      "Val loss: 0.5770491407058549, accuracy: 0.7791000371885459\n",
      "\n",
      "\n",
      "Epoch: 38\n",
      "Train loss: 0.6023313931347757, accuracy: 0.7817032354034957\n",
      "Val loss: 0.5722653275530948, accuracy: 0.7861658609148382\n",
      "\n",
      "\n",
      "Epoch: 39\n",
      "Train loss: 0.5378121407823786, accuracy: 0.8095946448493864\n",
      "Val loss: 0.5548111800147326, accuracy: 0.8040163629602083\n",
      "\n",
      "\n",
      "Epoch: 40\n",
      "Train loss: 0.5552319021817254, accuracy: 0.8029007065823727\n",
      "Val loss: 0.48701327207940537, accuracy: 0.8404611379695054\n",
      "\n",
      "\n",
      "Epoch: 41\n",
      "Train loss: 0.5876193816509332, accuracy: 0.7917441428040164\n",
      "Val loss: 0.5117128482535233, accuracy: 0.8203793231684641\n",
      "\n",
      "\n",
      "Epoch: 42\n",
      "Train loss: 0.5472476465545121, accuracy: 0.8025288211230941\n",
      "Val loss: 0.5565074871090896, accuracy: 0.801785050204537\n",
      "\n",
      "\n",
      "Epoch: 43\n",
      "Train loss: 0.5192011085284515, accuracy: 0.8162885831164002\n",
      "Val loss: 0.5141393449192111, accuracy: 0.815544812197843\n",
      "\n",
      "\n",
      "Epoch: 44\n",
      "Train loss: 0.5333087589160888, accuracy: 0.8095946448493864\n",
      "Val loss: 0.6716843793586357, accuracy: 0.7671997024916326\n",
      "\n",
      "\n",
      "Epoch: 45\n",
      "Train loss: 0.6276509985466259, accuracy: 0.7746374116772035\n",
      "Val loss: 0.5681979859039303, accuracy: 0.8073633320937151\n",
      "\n",
      "\n",
      "Epoch: 46\n",
      "Train loss: 0.5491903234410348, accuracy: 0.8051320193380439\n",
      "Val loss: 0.49096224251712023, accuracy: 0.8285608032725921\n",
      "\n",
      "\n",
      "Epoch: 47\n",
      "Train loss: 0.5845570957607477, accuracy: 0.788769059129788\n",
      "Val loss: 0.5380031389124378, accuracy: 0.8021569356638155\n",
      "\n",
      "\n",
      "Epoch: 48\n",
      "Train loss: 0.545037541193517, accuracy: 0.8006693938267013\n",
      "Val loss: 0.4762644683086726, accuracy: 0.8274451468947565\n",
      "\n",
      "\n",
      "Epoch: 49\n",
      "Train loss: 0.49065140797418927, accuracy: 0.8248419486798066\n",
      "Val loss: 0.5081709982870946, accuracy: 0.8211230940870212\n",
      "\n",
      "\n",
      "Epoch: 50\n",
      "Train loss: 0.5022302634789625, accuracy: 0.8162885831164002\n",
      "Val loss: 0.4271526398070021, accuracy: 0.8497582744514689\n",
      "\n",
      "\n",
      "Epoch: 51\n",
      "Train loss: 0.570314442149411, accuracy: 0.8010412792859799\n",
      "Val loss: 0.5357719724530312, accuracy: 0.815544812197843\n",
      "\n",
      "\n",
      "Epoch: 52\n",
      "Train loss: 0.5873332021182335, accuracy: 0.7872815172926738\n",
      "Val loss: 0.562308652576526, accuracy: 0.7805875790256601\n",
      "\n",
      "\n",
      "Epoch: 53\n",
      "Train loss: 0.5483947065116836, accuracy: 0.8081071030122722\n",
      "Val loss: 0.4804382094356286, accuracy: 0.8263294905169208\n",
      "\n",
      "\n",
      "Epoch: 54\n",
      "Train loss: 0.4882589111256307, accuracy: 0.8255857195983637\n",
      "Val loss: 0.4296068037119263, accuracy: 0.8512458162885831\n",
      "\n",
      "\n",
      "Epoch: 55\n",
      "Train loss: 0.4509289332396393, accuracy: 0.8434362216437338\n",
      "Val loss: 0.47839197346870677, accuracy: 0.8315358869468203\n",
      "\n",
      "\n",
      "Epoch: 56\n",
      "Train loss: 0.4255357841192546, accuracy: 0.8527333581256973\n",
      "Val loss: 0.3982179017260244, accuracy: 0.8560803272592041\n",
      "\n",
      "\n",
      "Epoch: 57\n",
      "Train loss: 0.5383095851659154, accuracy: 0.8110821866865006\n",
      "Val loss: 0.5822341782136139, accuracy: 0.7783562662699889\n",
      "\n",
      "\n",
      "Epoch: 58\n",
      "Train loss: 0.6704281466119837, accuracy: 0.7541837114168836\n",
      "Val loss: 0.8762627670693105, accuracy: 0.6749721085905541\n",
      "\n",
      "\n",
      "Epoch: 59\n",
      "Train loss: 0.628095453447815, accuracy: 0.7764968389735961\n",
      "Val loss: 0.4893741100283971, accuracy: 0.8322796578653775\n",
      "\n",
      "\n",
      "Epoch: 60\n",
      "Train loss: 0.46882482879930554, accuracy: 0.8311640014875419\n",
      "Val loss: 0.3895578042819802, accuracy: 0.8769059129788025\n",
      "\n",
      "\n",
      "Epoch: 61\n",
      "Train loss: 0.41248444266902684, accuracy: 0.8575678690963183\n",
      "Val loss: 0.4109952536468038, accuracy: 0.8534771290442543\n",
      "\n",
      "\n",
      "Epoch: 62\n",
      "Train loss: 0.3732320735138914, accuracy: 0.8728151729267386\n",
      "Val loss: 0.3115994738087241, accuracy: 0.898475269616958\n",
      "\n",
      "\n",
      "Epoch: 63\n",
      "Train loss: 0.33498859941937215, accuracy: 0.8862030494607661\n",
      "Val loss: 0.3316758527060608, accuracy: 0.8817404239494235\n",
      "\n",
      "\n",
      "Epoch: 64\n",
      "Train loss: 0.3505802883916559, accuracy: 0.871699516548903\n",
      "Val loss: 0.32219555059655, accuracy: 0.893640758646337\n",
      "\n",
      "\n",
      "Epoch: 65\n",
      "Train loss: 0.3108463011163901, accuracy: 0.8940126441056154\n",
      "Val loss: 0.31568239563324496, accuracy: 0.8962439568612868\n",
      "\n",
      "\n",
      "Epoch: 66\n",
      "Train loss: 0.36553298970410797, accuracy: 0.8687244328746746\n",
      "Val loss: 0.3184823965042131, accuracy: 0.8876905912978803\n",
      "\n",
      "\n",
      "Epoch: 67\n",
      "Train loss: 0.36311002678035936, accuracy: 0.8650055782818892\n",
      "Val loss: 0.3077567706723424, accuracy: 0.8958720714020082\n",
      "\n",
      "\n",
      "Epoch: 68\n",
      "Train loss: 0.3990691625282817, accuracy: 0.8512458162885831\n",
      "Val loss: 0.49983715441378573, accuracy: 0.8207512086277426\n",
      "\n",
      "\n",
      "Epoch: 69\n",
      "Train loss: 0.5541106014723404, accuracy: 0.8036444775009297\n",
      "Val loss: 0.46785758629125396, accuracy: 0.840833023428784\n",
      "\n",
      "\n",
      "Epoch: 70\n",
      "Train loss: 0.4882052708798157, accuracy: 0.823726292301971\n",
      "Val loss: 0.47608733711388085, accuracy: 0.8270732614354779\n",
      "\n",
      "\n",
      "Epoch: 71\n",
      "Train loss: 0.42407326928387346, accuracy: 0.8482707326143548\n",
      "Val loss: 0.33501689105549903, accuracy: 0.8832279657865377\n",
      "\n",
      "\n",
      "Epoch: 72\n",
      "Train loss: 0.3530164262139252, accuracy: 0.8713276310896244\n",
      "Val loss: 0.3206291218458298, accuracy: 0.893640758646337\n",
      "\n",
      "\n",
      "Epoch: 73\n",
      "Train loss: 0.3530403896085655, accuracy: 0.8743027147638527\n",
      "Val loss: 0.29092916929398205, accuracy: 0.9007065823726292\n",
      "\n",
      "\n",
      "Epoch: 74\n",
      "Train loss: 0.31903290049255717, accuracy: 0.8884343622164373\n",
      "Val loss: 0.31062052060609946, accuracy: 0.8899219040535515\n",
      "\n",
      "\n",
      "Epoch: 75\n",
      "Train loss: 0.3255915107027489, accuracy: 0.8876905912978803\n",
      "Val loss: 0.3397440969079522, accuracy: 0.8787653402751953\n",
      "\n",
      "\n",
      "Epoch: 76\n",
      "Train loss: 0.3254916610638862, accuracy: 0.8795091111937523\n",
      "Val loss: 0.2760929370644634, accuracy: 0.9070286351803645\n",
      "\n",
      "\n",
      "Epoch: 77\n",
      "Train loss: 0.2619997934532148, accuracy: 0.9096318333953143\n",
      "Val loss: 0.2834102481925031, accuracy: 0.8981033841576794\n",
      "\n",
      "\n",
      "Epoch: 78\n",
      "Train loss: 0.2898499095759103, accuracy: 0.895128300483451\n",
      "Val loss: 0.22644644355011412, accuracy: 0.9259947936035701\n",
      "\n",
      "\n",
      "Epoch: 79\n",
      "Train loss: 0.25366145553044467, accuracy: 0.9111193752324284\n",
      "Val loss: 0.23733084067264512, accuracy: 0.9133506879880997\n",
      "\n",
      "\n",
      "Epoch: 80\n",
      "Train loss: 0.2896574874844592, accuracy: 0.8891781331349944\n",
      "Val loss: 0.30306080756199855, accuracy: 0.8910375604313872\n",
      "\n",
      "\n",
      "Epoch: 81\n",
      "Train loss: 0.33566530336452355, accuracy: 0.8739308293045742\n",
      "Val loss: 0.35000851709366554, accuracy: 0.8787653402751953\n",
      "\n",
      "\n",
      "Epoch: 82\n",
      "Train loss: 0.3580545180900782, accuracy: 0.8709557456303458\n",
      "Val loss: 0.405923009317752, accuracy: 0.8597991818519896\n",
      "\n",
      "\n",
      "Epoch: 83\n",
      "Train loss: 0.4037601886605986, accuracy: 0.8601710673112681\n",
      "Val loss: 0.3675746156762372, accuracy: 0.8653774637411678\n",
      "\n",
      "\n",
      "Epoch: 84\n",
      "Train loss: 0.4046760582821925, accuracy: 0.8426924507251766\n",
      "Val loss: 0.33524660972823617, accuracy: 0.8906656749721086\n",
      "\n",
      "\n",
      "Epoch: 85\n",
      "Train loss: 0.331764285196022, accuracy: 0.8850873930829305\n",
      "Val loss: 0.28987406276778777, accuracy: 0.9059129788025289\n",
      "\n",
      "\n",
      "Epoch: 86\n",
      "Train loss: 0.30368190662441485, accuracy: 0.8973596132391224\n",
      "Val loss: 0.24273601937976721, accuracy: 0.9144663443659353\n",
      "\n",
      "\n",
      "Epoch: 87\n",
      "Train loss: 0.26890094395949965, accuracy: 0.9074005206396429\n",
      "Val loss: 0.29710697265485264, accuracy: 0.8914094458906657\n",
      "\n",
      "\n",
      "Epoch: 88\n",
      "Train loss: 0.32138578455192796, accuracy: 0.8873187058386017\n",
      "Val loss: 0.36066418602458605, accuracy: 0.867608776496839\n",
      "\n",
      "\n",
      "Epoch: 89\n",
      "Train loss: 0.30075799824802263, accuracy: 0.893640758646337\n",
      "Val loss: 0.23837355423168693, accuracy: 0.9140944589066567\n",
      "\n",
      "\n",
      "Epoch: 90\n",
      "Train loss: 0.24401991579691332, accuracy: 0.9088880624767571\n",
      "Val loss: 0.24467340296819423, accuracy: 0.9166976571216066\n",
      "\n",
      "\n",
      "Epoch: 91\n",
      "Train loss: 0.22140023451438398, accuracy: 0.9241353663071774\n",
      "Val loss: 0.1765227569506132, accuracy: 0.9438452956489402\n",
      "\n",
      "\n",
      "Epoch: 92\n",
      "Train loss: 0.19625803389350646, accuracy: 0.9312011900334697\n",
      "Val loss: 0.2529699460154087, accuracy: 0.9096318333953143\n",
      "\n",
      "\n",
      "Epoch: 93\n",
      "Train loss: 0.21088599728669052, accuracy: 0.9297136481963555\n",
      "Val loss: 0.2407547169988064, accuracy: 0.916325771662328\n",
      "\n",
      "\n",
      "Epoch: 94\n",
      "Train loss: 0.3254541983365924, accuracy: 0.8835998512458163\n",
      "Val loss: 0.5039030874294169, accuracy: 0.8281889178133135\n",
      "\n",
      "\n",
      "Epoch: 95\n",
      "Train loss: 0.35241963496302176, accuracy: 0.8832279657865377\n",
      "Val loss: 0.2910106032920531, accuracy: 0.8969877277798438\n",
      "\n",
      "\n",
      "Epoch: 96\n",
      "Train loss: 0.257787025065704, accuracy: 0.9103756043138713\n",
      "Val loss: 0.1797372941724338, accuracy: 0.9378951283004835\n",
      "\n",
      "\n",
      "Epoch: 97\n",
      "Train loss: 0.19643771453345296, accuracy: 0.9341762737076981\n",
      "Val loss: 0.16164826034568597, accuracy: 0.9497954629973968\n",
      "\n",
      "\n",
      "Epoch: 98\n",
      "Train loss: 0.21467474572895273, accuracy: 0.9300855336556341\n",
      "Val loss: 0.15335660505113038, accuracy: 0.9535143175901822\n",
      "\n",
      "\n",
      "Epoch: 99\n",
      "Train loss: 0.1688682386210391, accuracy: 0.9412420974339903\n",
      "Val loss: 0.18962377105316916, accuracy: 0.9438452956489402\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([2.5475490634084057,\n",
       "  2.0784351727739945,\n",
       "  1.769719611849205,\n",
       "  1.609328029544968,\n",
       "  1.543640068713153,\n",
       "  1.4766899859785545,\n",
       "  1.384676710031608,\n",
       "  1.34262731187004,\n",
       "  1.2118727995942364,\n",
       "  1.2028979649549083,\n",
       "  1.1282921889848805,\n",
       "  1.1230184295089682,\n",
       "  1.0164452210905472,\n",
       "  1.0698370933311054,\n",
       "  1.0028520067547635,\n",
       "  1.0143555294585165,\n",
       "  0.9880462660360354,\n",
       "  0.9331683500764449,\n",
       "  0.9485494461632342,\n",
       "  0.8469178006302339,\n",
       "  0.9137882608248425,\n",
       "  0.8341703537273868,\n",
       "  0.9390113525294868,\n",
       "  0.8437082525432753,\n",
       "  0.8478689237614465,\n",
       "  0.8953424286292627,\n",
       "  0.788359127464947,\n",
       "  0.8022378433943238,\n",
       "  0.7619840894857274,\n",
       "  0.7036853573631735,\n",
       "  0.8408173642756106,\n",
       "  0.7443200113738767,\n",
       "  0.704692486627876,\n",
       "  0.665764901951903,\n",
       "  0.7786017818582152,\n",
       "  0.6861973544974095,\n",
       "  0.6160871625146089,\n",
       "  0.5768301591371329,\n",
       "  0.6023313931347757,\n",
       "  0.5378121407823786,\n",
       "  0.5552319021817254,\n",
       "  0.5876193816509332,\n",
       "  0.5472476465545121,\n",
       "  0.5192011085284515,\n",
       "  0.5333087589160888,\n",
       "  0.6276509985466259,\n",
       "  0.5491903234410348,\n",
       "  0.5845570957607477,\n",
       "  0.545037541193517,\n",
       "  0.49065140797418927,\n",
       "  0.5022302634789625,\n",
       "  0.570314442149411,\n",
       "  0.5873332021182335,\n",
       "  0.5483947065116836,\n",
       "  0.4882589111256307,\n",
       "  0.4509289332396393,\n",
       "  0.4255357841192546,\n",
       "  0.5383095851659154,\n",
       "  0.6704281466119837,\n",
       "  0.628095453447815,\n",
       "  0.46882482879930554,\n",
       "  0.41248444266902684,\n",
       "  0.3732320735138914,\n",
       "  0.33498859941937215,\n",
       "  0.3505802883916559,\n",
       "  0.3108463011163901,\n",
       "  0.36553298970410797,\n",
       "  0.36311002678035936,\n",
       "  0.3990691625282817,\n",
       "  0.5541106014723404,\n",
       "  0.4882052708798157,\n",
       "  0.42407326928387346,\n",
       "  0.3530164262139252,\n",
       "  0.3530403896085655,\n",
       "  0.31903290049255717,\n",
       "  0.3255915107027489,\n",
       "  0.3254916610638862,\n",
       "  0.2619997934532148,\n",
       "  0.2898499095759103,\n",
       "  0.25366145553044467,\n",
       "  0.2896574874844592,\n",
       "  0.33566530336452355,\n",
       "  0.3580545180900782,\n",
       "  0.4037601886605986,\n",
       "  0.4046760582821925,\n",
       "  0.331764285196022,\n",
       "  0.30368190662441485,\n",
       "  0.26890094395949965,\n",
       "  0.32138578455192796,\n",
       "  0.30075799824802263,\n",
       "  0.24401991579691332,\n",
       "  0.22140023451438398,\n",
       "  0.19625803389350646,\n",
       "  0.21088599728669052,\n",
       "  0.3254541983365924,\n",
       "  0.35241963496302176,\n",
       "  0.257787025065704,\n",
       "  0.19643771453345296,\n",
       "  0.21467474572895273,\n",
       "  0.1688682386210391],\n",
       " [0.2026775753068055,\n",
       "  0.31238378579397547,\n",
       "  0.370397917441428,\n",
       "  0.4187430271476385,\n",
       "  0.42618073633320935,\n",
       "  0.4574191149126069,\n",
       "  0.5039047973224247,\n",
       "  0.523986612123466,\n",
       "  0.5656377835626627,\n",
       "  0.5734473782075121,\n",
       "  0.5961323912235031,\n",
       "  0.6121234659724805,\n",
       "  0.6507995537374489,\n",
       "  0.612495351431759,\n",
       "  0.6396429899590926,\n",
       "  0.6437337300111565,\n",
       "  0.6481963555224991,\n",
       "  0.6809222759390108,\n",
       "  0.6705094830792117,\n",
       "  0.7076980290070658,\n",
       "  0.6760877649683897,\n",
       "  0.7129044254369654,\n",
       "  0.6693938267013759,\n",
       "  0.7088136853849014,\n",
       "  0.7010040907400521,\n",
       "  0.678319077724061,\n",
       "  0.7155076236519152,\n",
       "  0.7095574563034586,\n",
       "  0.7314986984008925,\n",
       "  0.7493492004462625,\n",
       "  0.7017478616586091,\n",
       "  0.7415396058014132,\n",
       "  0.7500929713648197,\n",
       "  0.766084046113797,\n",
       "  0.7147638527333581,\n",
       "  0.7623651915210116,\n",
       "  0.7835626626998884,\n",
       "  0.7939754555596876,\n",
       "  0.7817032354034957,\n",
       "  0.8095946448493864,\n",
       "  0.8029007065823727,\n",
       "  0.7917441428040164,\n",
       "  0.8025288211230941,\n",
       "  0.8162885831164002,\n",
       "  0.8095946448493864,\n",
       "  0.7746374116772035,\n",
       "  0.8051320193380439,\n",
       "  0.788769059129788,\n",
       "  0.8006693938267013,\n",
       "  0.8248419486798066,\n",
       "  0.8162885831164002,\n",
       "  0.8010412792859799,\n",
       "  0.7872815172926738,\n",
       "  0.8081071030122722,\n",
       "  0.8255857195983637,\n",
       "  0.8434362216437338,\n",
       "  0.8527333581256973,\n",
       "  0.8110821866865006,\n",
       "  0.7541837114168836,\n",
       "  0.7764968389735961,\n",
       "  0.8311640014875419,\n",
       "  0.8575678690963183,\n",
       "  0.8728151729267386,\n",
       "  0.8862030494607661,\n",
       "  0.871699516548903,\n",
       "  0.8940126441056154,\n",
       "  0.8687244328746746,\n",
       "  0.8650055782818892,\n",
       "  0.8512458162885831,\n",
       "  0.8036444775009297,\n",
       "  0.823726292301971,\n",
       "  0.8482707326143548,\n",
       "  0.8713276310896244,\n",
       "  0.8743027147638527,\n",
       "  0.8884343622164373,\n",
       "  0.8876905912978803,\n",
       "  0.8795091111937523,\n",
       "  0.9096318333953143,\n",
       "  0.895128300483451,\n",
       "  0.9111193752324284,\n",
       "  0.8891781331349944,\n",
       "  0.8739308293045742,\n",
       "  0.8709557456303458,\n",
       "  0.8601710673112681,\n",
       "  0.8426924507251766,\n",
       "  0.8850873930829305,\n",
       "  0.8973596132391224,\n",
       "  0.9074005206396429,\n",
       "  0.8873187058386017,\n",
       "  0.893640758646337,\n",
       "  0.9088880624767571,\n",
       "  0.9241353663071774,\n",
       "  0.9312011900334697,\n",
       "  0.9297136481963555,\n",
       "  0.8835998512458163,\n",
       "  0.8832279657865377,\n",
       "  0.9103756043138713,\n",
       "  0.9341762737076981,\n",
       "  0.9300855336556341,\n",
       "  0.9412420974339903],\n",
       " [2.3049066645720493,\n",
       "  1.8235280966838525,\n",
       "  1.6978346623200473,\n",
       "  1.5720235032634515,\n",
       "  1.4766992452908023,\n",
       "  1.3395355590084044,\n",
       "  1.3723277179846403,\n",
       "  1.1941352069666058,\n",
       "  1.2032534022808252,\n",
       "  1.1740639662822412,\n",
       "  1.0772436972487234,\n",
       "  1.01723581975878,\n",
       "  1.0338206198544428,\n",
       "  1.1051030071218824,\n",
       "  0.9136324910347947,\n",
       "  1.1478215445812505,\n",
       "  0.9288133655440279,\n",
       "  0.9966861669260935,\n",
       "  0.8647939216341393,\n",
       "  0.8386954297005088,\n",
       "  0.878749091858401,\n",
       "  0.8970961866878852,\n",
       "  0.8521829601910621,\n",
       "  0.7785365855263795,\n",
       "  0.7567763167183033,\n",
       "  0.7485695902234606,\n",
       "  0.8530128964397179,\n",
       "  0.7833080519393901,\n",
       "  0.6757838788552105,\n",
       "  0.8180379132525811,\n",
       "  0.6704310613212288,\n",
       "  0.6552367725085042,\n",
       "  0.6838663376676588,\n",
       "  0.7756223816018691,\n",
       "  0.7294749659913852,\n",
       "  0.6381594214176946,\n",
       "  0.5992389828044271,\n",
       "  0.5770491407058549,\n",
       "  0.5722653275530948,\n",
       "  0.5548111800147326,\n",
       "  0.48701327207940537,\n",
       "  0.5117128482535233,\n",
       "  0.5565074871090896,\n",
       "  0.5141393449192111,\n",
       "  0.6716843793586357,\n",
       "  0.5681979859039303,\n",
       "  0.49096224251712023,\n",
       "  0.5380031389124378,\n",
       "  0.4762644683086726,\n",
       "  0.5081709982870946,\n",
       "  0.4271526398070021,\n",
       "  0.5357719724530312,\n",
       "  0.562308652576526,\n",
       "  0.4804382094356286,\n",
       "  0.4296068037119263,\n",
       "  0.47839197346870677,\n",
       "  0.3982179017260244,\n",
       "  0.5822341782136139,\n",
       "  0.8762627670693105,\n",
       "  0.4893741100283971,\n",
       "  0.3895578042819802,\n",
       "  0.4109952536468038,\n",
       "  0.3115994738087241,\n",
       "  0.3316758527060608,\n",
       "  0.32219555059655,\n",
       "  0.31568239563324496,\n",
       "  0.3184823965042131,\n",
       "  0.3077567706723424,\n",
       "  0.49983715441378573,\n",
       "  0.46785758629125396,\n",
       "  0.47608733711388085,\n",
       "  0.33501689105549903,\n",
       "  0.3206291218458298,\n",
       "  0.29092916929398205,\n",
       "  0.31062052060609946,\n",
       "  0.3397440969079522,\n",
       "  0.2760929370644634,\n",
       "  0.2834102481925031,\n",
       "  0.22644644355011412,\n",
       "  0.23733084067264512,\n",
       "  0.30306080756199855,\n",
       "  0.35000851709366554,\n",
       "  0.405923009317752,\n",
       "  0.3675746156762372,\n",
       "  0.33524660972823617,\n",
       "  0.28987406276778777,\n",
       "  0.24273601937976721,\n",
       "  0.29710697265485264,\n",
       "  0.36066418602458605,\n",
       "  0.23837355423168693,\n",
       "  0.24467340296819423,\n",
       "  0.1765227569506132,\n",
       "  0.2529699460154087,\n",
       "  0.2407547169988064,\n",
       "  0.5039030874294169,\n",
       "  0.2910106032920531,\n",
       "  0.1797372941724338,\n",
       "  0.16164826034568597,\n",
       "  0.15335660505113038,\n",
       "  0.18962377105316916],\n",
       " [0.27891409445890664,\n",
       "  0.39085161770174787,\n",
       "  0.39791744142804014,\n",
       "  0.4369654146522871,\n",
       "  0.4696913350687988,\n",
       "  0.5113425065079955,\n",
       "  0.49014503532911863,\n",
       "  0.5738192636667906,\n",
       "  0.5838601710673113,\n",
       "  0.5760505764224619,\n",
       "  0.6355522499070286,\n",
       "  0.6284864261807364,\n",
       "  0.602826329490517,\n",
       "  0.5905541093343251,\n",
       "  0.6838973596132392,\n",
       "  0.5619189289698773,\n",
       "  0.6507995537374489,\n",
       "  0.6507995537374489,\n",
       "  0.6980290070658237,\n",
       "  0.6991446634436593,\n",
       "  0.687988099665303,\n",
       "  0.6723689103756043,\n",
       "  0.6987727779843809,\n",
       "  0.7385645221271848,\n",
       "  0.7378207512086278,\n",
       "  0.7348456675343994,\n",
       "  0.687244328746746,\n",
       "  0.7229453328374861,\n",
       "  0.7701747861658609,\n",
       "  0.7173670509483079,\n",
       "  0.761249535143176,\n",
       "  0.7731498698400893,\n",
       "  0.7616214206024544,\n",
       "  0.7032354034957233,\n",
       "  0.7456303458534771,\n",
       "  0.7779843808107103,\n",
       "  0.7794719226478245,\n",
       "  0.7791000371885459,\n",
       "  0.7861658609148382,\n",
       "  0.8040163629602083,\n",
       "  0.8404611379695054,\n",
       "  0.8203793231684641,\n",
       "  0.801785050204537,\n",
       "  0.815544812197843,\n",
       "  0.7671997024916326,\n",
       "  0.8073633320937151,\n",
       "  0.8285608032725921,\n",
       "  0.8021569356638155,\n",
       "  0.8274451468947565,\n",
       "  0.8211230940870212,\n",
       "  0.8497582744514689,\n",
       "  0.815544812197843,\n",
       "  0.7805875790256601,\n",
       "  0.8263294905169208,\n",
       "  0.8512458162885831,\n",
       "  0.8315358869468203,\n",
       "  0.8560803272592041,\n",
       "  0.7783562662699889,\n",
       "  0.6749721085905541,\n",
       "  0.8322796578653775,\n",
       "  0.8769059129788025,\n",
       "  0.8534771290442543,\n",
       "  0.898475269616958,\n",
       "  0.8817404239494235,\n",
       "  0.893640758646337,\n",
       "  0.8962439568612868,\n",
       "  0.8876905912978803,\n",
       "  0.8958720714020082,\n",
       "  0.8207512086277426,\n",
       "  0.840833023428784,\n",
       "  0.8270732614354779,\n",
       "  0.8832279657865377,\n",
       "  0.893640758646337,\n",
       "  0.9007065823726292,\n",
       "  0.8899219040535515,\n",
       "  0.8787653402751953,\n",
       "  0.9070286351803645,\n",
       "  0.8981033841576794,\n",
       "  0.9259947936035701,\n",
       "  0.9133506879880997,\n",
       "  0.8910375604313872,\n",
       "  0.8787653402751953,\n",
       "  0.8597991818519896,\n",
       "  0.8653774637411678,\n",
       "  0.8906656749721086,\n",
       "  0.9059129788025289,\n",
       "  0.9144663443659353,\n",
       "  0.8914094458906657,\n",
       "  0.867608776496839,\n",
       "  0.9140944589066567,\n",
       "  0.9166976571216066,\n",
       "  0.9438452956489402,\n",
       "  0.9096318333953143,\n",
       "  0.916325771662328,\n",
       "  0.8281889178133135,\n",
       "  0.8969877277798438,\n",
       "  0.9378951283004835,\n",
       "  0.9497954629973968,\n",
       "  0.9535143175901822,\n",
       "  0.9438452956489402])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model,\n",
    "            optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            n_epoch=100,\n",
    "            batch_size=200,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            backup_name=\"lstm_action_classifier.pth.tar\")"
   ]
  },
  {
   "source": [
    "## Load and Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"lstm_action_classifier.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_sequence(model, sequence):\n",
    "    model.eval()\n",
    "    X_batch = sequence.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model.forward(X_batch)\n",
    "        print(y_pred)\n",
    "    return torch.argmax(y_pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 7.6807, -7.4713, -1.3673, -8.8626, -7.3632, -1.4662]],\n       device='cuda:0')\n0\n"
     ]
    }
   ],
   "source": [
    "sequence = train_dataset[45][0].unsqueeze(0)\n",
    "predictions = run_inference_on_sequence(model, sequence)\n",
    "print(predictions.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 6.3528, -7.8725, -2.6291, -6.6467, -6.0407, -5.5711]],\n       device='cuda:0')\n0\n"
     ]
    }
   ],
   "source": [
    "predictions = run_inference_on_sequence(model, sequence)\n",
    "print(predictions.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = ([0.8735486226845841,\n",
    "  0.48130274497343034,\n",
    "  0.3702782436299719,\n",
    "  0.3491351202047991,\n",
    "  0.2769810041340675,\n",
    "  0.20154464583205914,\n",
    "  0.2445332597879415,\n",
    "  0.17224384997431086,\n",
    "  0.1885981674023096,\n",
    "  0.19384880530241444,\n",
    "  0.19337554822639866,\n",
    "  0.16536105377239416,\n",
    "  0.1571138994552154,\n",
    "  0.15273290199140158,\n",
    "  0.10815980746436514,\n",
    "  0.12133629483412643,\n",
    "  0.13833611768088946,\n",
    "  0.1293170387481046,\n",
    "  0.10733797376491747,\n",
    "  0.10590762594786797,\n",
    "  0.0918239034835805,\n",
    "  0.1047938061950121,\n",
    "  0.07058675783115197,\n",
    "  0.08766820735364987,\n",
    "  0.10277271190071632,\n",
    "  0.12322432884653176,\n",
    "  0.08019538524956828,\n",
    "  0.08205282129964776,\n",
    "  0.07426594978620334,\n",
    "  0.0755969595853548,\n",
    "  0.09760807308867492,\n",
    "  0.07207354542732404,\n",
    "  0.0975812500682325,\n",
    "  0.09537964315045605,\n",
    "  0.09366875433450493,\n",
    "  0.04696851966758542,\n",
    "  0.0512240380254569,\n",
    "  0.05459230239451631,\n",
    "  0.08089525441829507,\n",
    "  0.06131623744944018,\n",
    "  0.06199158364649636,\n",
    "  0.051481724839184166,\n",
    "  0.06538778840672245,\n",
    "  0.0629294374865063,\n",
    "  0.04425359480803349,\n",
    "  0.05855927577252546,\n",
    "  0.08699687327160525,\n",
    "  0.07487187519443134,\n",
    "  0.05951084620550851,\n",
    "  0.05211047360593793,\n",
    "  0.0654746382687632,\n",
    "  0.07681369263658207,\n",
    "  0.044812741151172154,\n",
    "  0.04169330514596971,\n",
    "  0.056397179180745924,\n",
    "  0.06540772032921424,\n",
    "  0.07633163423087057,\n",
    "  0.055633779746886776,\n",
    "  0.06838037114775641,\n",
    "  0.053590912112544256,\n",
    "  0.0628714390222539,\n",
    "  0.045393846311621903,\n",
    "  0.05751636609517408,\n",
    "  0.08074402875827821,\n",
    "  0.04321444615804029,\n",
    "  0.05079126861872594,\n",
    "  0.050103869856186006,\n",
    "  0.03507644276016325,\n",
    "  0.03688339269977289,\n",
    "  0.041359042481246576,\n",
    "  0.03544109247172077,\n",
    "  0.03485896200435596,\n",
    "  0.04210835051470699,\n",
    "  0.07559601816384749,\n",
    "  0.04357416177816813,\n",
    "  0.030860471100808837,\n",
    "  0.036541297235294604,\n",
    "  0.03869030817890365,\n",
    "  0.036192047033067894,\n",
    "  0.05795359878571838,\n",
    "  0.040659130261747875,\n",
    "  0.04481880535572273,\n",
    "  0.06331975865306445,\n",
    "  0.04256203787514518,\n",
    "  0.03762714751888046,\n",
    "  0.0230426168100116,\n",
    "  0.03029545205979716,\n",
    "  0.04663788862835142,\n",
    "  0.04618885129772497,\n",
    "  0.06090077068450866,\n",
    "  0.05378697729046914,\n",
    "  0.05114451131438682,\n",
    "  0.0467184322129791,\n",
    "  0.05389563671920387,\n",
    "  0.048165335642040105,\n",
    "  0.04513653215453111,\n",
    "  0.037053695992046956,\n",
    "  0.07355283905870348,\n",
    "  0.05021775581395428,\n",
    "  0.05387003534121017],\n",
    " [0.6489281767955801,\n",
    "  0.8121988950276243,\n",
    "  0.8585193370165746,\n",
    "  0.8681546961325967,\n",
    "  0.891889502762431,\n",
    "  0.9223425414364641,\n",
    "  0.906121546961326,\n",
    "  0.9344530386740332,\n",
    "  0.9319779005524862,\n",
    "  0.927646408839779,\n",
    "  0.925878453038674,\n",
    "  0.9390055248618785,\n",
    "  0.9413480662983426,\n",
    "  0.9398453038674033,\n",
    "  0.9611491712707182,\n",
    "  0.9558895027624309,\n",
    "  0.9517348066298342,\n",
    "  0.9534585635359116,\n",
    "  0.9620773480662983,\n",
    "  0.9622099447513812,\n",
    "  0.9653038674033149,\n",
    "  0.9622541436464088,\n",
    "  0.9747624309392265,\n",
    "  0.9688397790055249,\n",
    "  0.963403314917127,\n",
    "  0.9549613259668508,\n",
    "  0.9732154696132597,\n",
    "  0.9703867403314917,\n",
    "  0.974232044198895,\n",
    "  0.9735690607734807,\n",
    "  0.9656574585635359,\n",
    "  0.9750276243093923,\n",
    "  0.964817679558011,\n",
    "  0.9656132596685083,\n",
    "  0.9657016574585635,\n",
    "  0.9833370165745856,\n",
    "  0.9824530386740331,\n",
    "  0.9800220994475138,\n",
    "  0.9707845303867403,\n",
    "  0.9776795580110498,\n",
    "  0.9775027624309393,\n",
    "  0.9813480662983426,\n",
    "  0.9775911602209945,\n",
    "  0.9772375690607735,\n",
    "  0.9844861878453038,\n",
    "  0.9794475138121547,\n",
    "  0.9695469613259668,\n",
    "  0.9748066298342541,\n",
    "  0.9797569060773481,\n",
    "  0.981524861878453,\n",
    "  0.9777237569060774,\n",
    "  0.9734806629834254,\n",
    "  0.9838232044198895,\n",
    "  0.9848397790055249,\n",
    "  0.980243093922652,\n",
    "  0.9768839779005525,\n",
    "  0.9737016574585635,\n",
    "  0.9805966850828729,\n",
    "  0.9762651933701657,\n",
    "  0.9809060773480663,\n",
    "  0.978342541436464,\n",
    "  0.9840441988950276,\n",
    "  0.97953591160221,\n",
    "  0.9716685082872928,\n",
    "  0.9851491712707182,\n",
    "  0.9826298342541436,\n",
    "  0.9824972375690608,\n",
    "  0.9877127071823204,\n",
    "  0.9868729281767956,\n",
    "  0.9853259668508287,\n",
    "  0.9874475138121547,\n",
    "  0.9878453038674033,\n",
    "  0.9854143646408839,\n",
    "  0.9754696132596685,\n",
    "  0.9849723756906077,\n",
    "  0.9887734806629834,\n",
    "  0.9881546961325967,\n",
    "  0.9861215469613259,\n",
    "  0.9877569060773481,\n",
    "  0.9792707182320441,\n",
    "  0.9848397790055249,\n",
    "  0.984707182320442,\n",
    "  0.9776795580110498,\n",
    "  0.984707182320442,\n",
    "  0.9879779005524861,\n",
    "  0.9927513812154696,\n",
    "  0.9897458563535911,\n",
    "  0.9848839779005525,\n",
    "  0.9849281767955801,\n",
    "  0.9796685082872928,\n",
    "  0.9809502762430939,\n",
    "  0.9824530386740331,\n",
    "  0.9832044198895028,\n",
    "  0.9803314917127072,\n",
    "  0.9834254143646409,\n",
    "  0.9847513812154696,\n",
    "  0.9872265193370166,\n",
    "  0.9749834254143647,\n",
    "  0.9823204419889503,\n",
    "  0.982718232044199],\n",
    " [0.6211841620039762,\n",
    "  0.45876633668316125,\n",
    "  0.5034662859063835,\n",
    "  0.27011767895693073,\n",
    "  0.313655665158899,\n",
    "  0.20805899514211246,\n",
    "  0.3991670820106114,\n",
    "  0.32158903264995403,\n",
    "  0.24071570668090958,\n",
    "  0.2567666179594932,\n",
    "  0.3852716677423312,\n",
    "  0.4597050237921575,\n",
    "  0.17627169369967327,\n",
    "  0.23679502981565434,\n",
    "  0.17220754785561754,\n",
    "  0.18002146034804078,\n",
    "  0.22931813937735357,\n",
    "  0.16827500484832936,\n",
    "  0.15206738518692042,\n",
    "  0.14282209915193997,\n",
    "  0.18685480398813387,\n",
    "  0.11685570925779426,\n",
    "  0.20067864179665781,\n",
    "  0.1411727972821085,\n",
    "  0.1530777178196244,\n",
    "  0.13876111770846125,\n",
    "  0.14196548254774952,\n",
    "  0.2708777552003365,\n",
    "  0.11546770097299551,\n",
    "  0.3046354775860649,\n",
    "  0.10854558102802168,\n",
    "  0.07933161259128273,\n",
    "  0.12404691129011139,\n",
    "  0.38784327351626957,\n",
    "  0.07989218154476951,\n",
    "  0.17622217972792195,\n",
    "  0.09162371938435548,\n",
    "  0.13405118411540942,\n",
    "  0.10392435214123062,\n",
    "  0.10115272597895769,\n",
    "  0.09087322915753134,\n",
    "  0.1590506696334804,\n",
    "  0.22317246896386603,\n",
    "  0.08073770713501655,\n",
    "  0.058100116569523644,\n",
    "  0.14888879645115743,\n",
    "  0.13044291378562928,\n",
    "  0.07736094586014841,\n",
    "  0.11220023536174023,\n",
    "  0.13659915878864518,\n",
    "  0.07129768802977254,\n",
    "  0.20124599370539134,\n",
    "  0.07354985906765003,\n",
    "  0.08835384215030306,\n",
    "  0.21074753585173564,\n",
    "  0.09616019399096185,\n",
    "  0.11539241931760864,\n",
    "  0.120142063001071,\n",
    "  0.0775641988276421,\n",
    "  0.09695740938079764,\n",
    "  0.09058009887954965,\n",
    "  0.06536958760441548,\n",
    "  0.14220434166765117,\n",
    "  0.07930320302022702,\n",
    "  0.08943844097148232,\n",
    "  0.06493218757685769,\n",
    "  0.08265407143692353,\n",
    "  0.0510503365727251,\n",
    "  0.06409464690922057,\n",
    "  0.0902351198168243,\n",
    "  0.06698930089201968,\n",
    "  0.07941066838737426,\n",
    "  0.27309605946031246,\n",
    "  0.10141229745728485,\n",
    "  0.08438794725363463,\n",
    "  0.06564738088682306,\n",
    "  0.07934415308124455,\n",
    "  0.11631594676814944,\n",
    "  0.11903673065906457,\n",
    "  0.09058221255546416,\n",
    "  0.12191057274405673,\n",
    "  0.1233115003656721,\n",
    "  0.10492566676817347,\n",
    "  0.11040246008239304,\n",
    "  0.0784957536592911,\n",
    "  0.05796452499581156,\n",
    "  0.06605394684588234,\n",
    "  0.08701971364890956,\n",
    "  0.23499713032428876,\n",
    "  0.14679802382028134,\n",
    "  0.0931847726414798,\n",
    "  0.1281534519503502,\n",
    "  0.09721326736259316,\n",
    "  0.0960857172550666,\n",
    "  0.1704642465234064,\n",
    "  0.11685481813445657,\n",
    "  0.14966437045596195,\n",
    "  0.22490323309782892,\n",
    "  0.09092609374476246,\n",
    "  0.08504296933522287],\n",
    " [0.7440445139975657,\n",
    "  0.8125543383759346,\n",
    "  0.8101199791340636,\n",
    "  0.9012345679012346,\n",
    "  0.869414014953921,\n",
    "  0.9231438010780734,\n",
    "  0.8497652582159625,\n",
    "  0.892018779342723,\n",
    "  0.9128847157016171,\n",
    "  0.9021039819161885,\n",
    "  0.8753260302556077,\n",
    "  0.8575899843505478,\n",
    "  0.9285341679707877,\n",
    "  0.9189706138062945,\n",
    "  0.9384454877412624,\n",
    "  0.9398365501651886,\n",
    "  0.9163623717614328,\n",
    "  0.94679186228482,\n",
    "  0.9419231438010781,\n",
    "  0.942792557816032,\n",
    "  0.9297513475917232,\n",
    "  0.952529994783516,\n",
    "  0.9400104329681794,\n",
    "  0.9497478699356634,\n",
    "  0.9521822291775344,\n",
    "  0.9539210572074421,\n",
    "  0.9410537297861241,\n",
    "  0.9031472787341331,\n",
    "  0.9601808381151105,\n",
    "  0.8941053729786124,\n",
    "  0.9612241349330551,\n",
    "  0.9688749782646496,\n",
    "  0.9575725960702487,\n",
    "  0.8626325856372805,\n",
    "  0.9680055642496957,\n",
    "  0.9514866979655712,\n",
    "  0.9640062597809077,\n",
    "  0.952529994783516,\n",
    "  0.9575725960702487,\n",
    "  0.9641801425838985,\n",
    "  0.9650495565988524,\n",
    "  0.9384454877412624,\n",
    "  0.9297513475917232,\n",
    "  0.9683533298556772,\n",
    "  0.9746131107633456,\n",
    "  0.9436619718309859,\n",
    "  0.9544427056164145,\n",
    "  0.9688749782646496,\n",
    "  0.9613980177360459,\n",
    "  0.9547904712223961,\n",
    "  0.9721787515214745,\n",
    "  0.9337506520605112,\n",
    "  0.9754825247782994,\n",
    "  0.968527212658668,\n",
    "  0.9198400278212485,\n",
    "  0.9629629629629629,\n",
    "  0.9573987132672579,\n",
    "  0.9657450878108155,\n",
    "  0.97339593114241,\n",
    "  0.96452790818988,\n",
    "  0.9657450878108155,\n",
    "  0.9735698139454008,\n",
    "  0.9497478699356634,\n",
    "  0.9697443922796035,\n",
    "  0.9680055642496957,\n",
    "  0.9721787515214745,\n",
    "  0.970961571900539,\n",
    "  0.9780907668231612,\n",
    "  0.974960876369327,\n",
    "  0.9699182750825943,\n",
    "  0.9723526343244653,\n",
    "  0.9711354547035298,\n",
    "  0.9167101373674144,\n",
    "  0.9673100330377326,\n",
    "  0.9711354547035298,\n",
    "  0.9798295948530691,\n",
    "  0.9725265171274561,\n",
    "  0.9643540253868892,\n",
    "  0.9551382368283776,\n",
    "  0.9697443922796035,\n",
    "  0.9587897756911842,\n",
    "  0.9591375412971657,\n",
    "  0.970961571900539,\n",
    "  0.9648756737958616,\n",
    "  0.9720048687184838,\n",
    "  0.9810467744740046,\n",
    "  0.975830290384281,\n",
    "  0.9728742827334377,\n",
    "  0.9300991131977048,\n",
    "  0.9553121196313684,\n",
    "  0.9706138062945575,\n",
    "  0.9601808381151105,\n",
    "  0.9674839158407234,\n",
    "  0.9673100330377326,\n",
    "  0.9394887845592071,\n",
    "  0.9636584941749261,\n",
    "  0.9558337680403408,\n",
    "  0.9257520431229351,\n",
    "  0.9659189706138063,\n",
    "  0.970961571900539])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}