{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \" + DEVICE)\n",
    "if torch.backends.cudnn.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_with_norm(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, header=None))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, header=None))\n",
    "    \n",
    "    # Shift labels in PyTorch classification models labels start from 0\n",
    "    y_data = y_data - 1\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, sep=\"\\t\", header=None, dtype=np.float32))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, sep=\"\\t\", header=None, dtype=np.int_))\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 16\n",
    "\n",
    "X_all, y_all = read_data(\"data/shuffled_poses.csv\",\"data/shuffled_labels.csv\", SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5378"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "val_size = int(y_all.shape[0] * 0.2)\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = X_all[-val_size:], y_all[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_all[:-val_size], y_all[:-val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5378, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(21514, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                               torch.tensor(y_train, dtype=torch.long).squeeze())\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                             torch.tensor(y_val, dtype=torch.long).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_classes, lstm_hidden_dim=256, fc_hidden_dim=256, n_lstm_layers=2):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        \n",
    "        self._lstm = nn.LSTM(input_size=input_dim,\n",
    "                             hidden_size=lstm_hidden_dim,\n",
    "                             num_layers=n_lstm_layers,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        self._fc = nn.Sequential(nn.Linear(lstm_hidden_dim, fc_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc_hidden_dim, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self._lstm.forward(x)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        fc_output = self._fc.forward(lstm_output)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, criterion, batches, phase='train'):\n",
    "    is_train = phase == 'train'\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_predictions = 0\n",
    "    \n",
    "    correct_predictions = 0\n",
    "\n",
    "    for X_batch, y_batch in batches:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = criterion.forward(y_pred, y_batch)\n",
    "    \n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * y_batch.shape[0]\n",
    "        correct_predictions += (torch.argmax(y_pred, dim=1) == y_batch).sum().item()\n",
    "        n_predictions += y_batch.shape[0]\n",
    "\n",
    "    epoch_loss = epoch_loss / n_predictions\n",
    "    epoch_accuracy = correct_predictions / n_predictions\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, n_epoch, batch_size, train_dataset, val_dataset, backup_name):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    train_batches = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    val_batches = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_accuracy = run_epoch(model, optimizer, criterion, train_batches, phase='train')\n",
    "        val_loss, val_accuracy = run_epoch(model, optimizer, criterion, val_batches, phase='val')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), backup_name)\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        print(\"Train loss: \" + str(train_loss) + \", accuracy: \" + str(train_accuracy))\n",
    "        print(\"Val loss: \" + str(val_loss) + \", accuracy: \" + str(val_accuracy) + \"\\n\\n\")\n",
    "        \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "source": [
    "N_CLASSES = 15\n",
    "INPUT_DIM = 36"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "245, accuracy: 0.8254624895416938\n",
      "Val loss: 0.8465989427346641, accuracy: 0.7326143547787282\n",
      "\n",
      "\n",
      "Epoch: 50\n",
      "Train loss: 0.4702070959191501, accuracy: 0.8328065445756252\n",
      "Val loss: 0.7674094970095952, accuracy: 0.7632949051692078\n",
      "\n",
      "\n",
      "Epoch: 51\n",
      "Train loss: 0.5347055805054165, accuracy: 0.8145858510737194\n",
      "Val loss: 0.7525547074757353, accuracy: 0.7499070286351803\n",
      "\n",
      "\n",
      "Epoch: 52\n",
      "Train loss: 0.427522484095021, accuracy: 0.8479594682532304\n",
      "Val loss: 0.8317528452382029, accuracy: 0.7463741167720342\n",
      "\n",
      "\n",
      "Epoch: 53\n",
      "Train loss: 0.4104117920925773, accuracy: 0.8542809333457284\n",
      "Val loss: 0.8280942385504352, accuracy: 0.7424693194496095\n",
      "\n",
      "\n",
      "Epoch: 54\n",
      "Train loss: 0.44079722174151276, accuracy: 0.845635400204518\n",
      "Val loss: 0.793187925943259, accuracy: 0.7525102268501301\n",
      "\n",
      "\n",
      "Epoch: 55\n",
      "Train loss: 0.48292121589543974, accuracy: 0.8282513712001487\n",
      "Val loss: 0.8219035819544142, accuracy: 0.7458162885831164\n",
      "\n",
      "\n",
      "Epoch: 56\n",
      "Train loss: 0.3940741437412587, accuracy: 0.860555917077252\n",
      "Val loss: 0.7792159010593489, accuracy: 0.7738936407586463\n",
      "\n",
      "\n",
      "Epoch: 57\n",
      "Train loss: 0.36704946573842046, accuracy: 0.8665984940039044\n",
      "Val loss: 0.804285173236149, accuracy: 0.7573447378207512\n",
      "\n",
      "\n",
      "Epoch: 58\n",
      "Train loss: 0.4269925632657, accuracy: 0.8472622478386167\n",
      "Val loss: 0.7074452617346642, accuracy: 0.7831907772406099\n",
      "\n",
      "\n",
      "Epoch: 59\n",
      "Train loss: 0.4194475779432983, accuracy: 0.8509342753555824\n",
      "Val loss: 0.767826415429376, accuracy: 0.7651543324656006\n",
      "\n",
      "\n",
      "Epoch: 60\n",
      "Train loss: 0.3706787081636704, accuracy: 0.8699451519940504\n",
      "Val loss: 0.740374417476115, accuracy: 0.7658981033841576\n",
      "\n",
      "\n",
      "Epoch: 61\n",
      "Train loss: 0.3964790462549927, accuracy: 0.8587431439992563\n",
      "Val loss: 0.8436707541950577, accuracy: 0.7439568612867237\n",
      "\n",
      "\n",
      "Epoch: 62\n",
      "Train loss: 0.39281501252045914, accuracy: 0.8583712931114623\n",
      "Val loss: 0.7690808913352188, accuracy: 0.7582744514689476\n",
      "\n",
      "\n",
      "Epoch: 63\n",
      "Train loss: 0.3872052518211245, accuracy: 0.8653899786185739\n",
      "Val loss: 0.770382628515335, accuracy: 0.7761249535143175\n",
      "\n",
      "\n",
      "Epoch: 64\n",
      "Train loss: 0.37693397522504596, accuracy: 0.8655294227014967\n",
      "Val loss: 0.8205424699476547, accuracy: 0.7534399404983265\n",
      "\n",
      "\n",
      "Epoch: 65\n",
      "Train loss: 0.41833758605194765, accuracy: 0.8534442688481919\n",
      "Val loss: 0.8645832958874006, accuracy: 0.7415396058014132\n",
      "\n",
      "\n",
      "Epoch: 66\n",
      "Train loss: 0.32914552750270076, accuracy: 0.8858417774472437\n",
      "Val loss: 0.7221817919515509, accuracy: 0.7870955745630346\n",
      "\n",
      "\n",
      "Epoch: 67\n",
      "Train loss: 0.2967928742677004, accuracy: 0.8928139815933811\n",
      "Val loss: 0.7534581502825409, accuracy: 0.7731498698400893\n",
      "\n",
      "\n",
      "Epoch: 68\n",
      "Train loss: 0.3429090168254021, accuracy: 0.8784047596913638\n",
      "Val loss: 0.6879422347223773, accuracy: 0.8014131647452585\n",
      "\n",
      "\n",
      "Epoch: 69\n",
      "Train loss: 0.28500870745111334, accuracy: 0.8999721111834155\n",
      "Val loss: 0.7191299769283449, accuracy: 0.7869096318333954\n",
      "\n",
      "\n",
      "Epoch: 70\n",
      "Train loss: 0.320230091311868, accuracy: 0.8889560286325183\n",
      "Val loss: 0.7877149536025351, accuracy: 0.7685013015991075\n",
      "\n",
      "\n",
      "Epoch: 71\n",
      "Train loss: 0.3508410321781823, accuracy: 0.8758018034768058\n",
      "Val loss: 0.7584751909131142, accuracy: 0.7781703235403495\n",
      "\n",
      "\n",
      "Epoch: 72\n",
      "Train loss: 0.2840117776167817, accuracy: 0.9006228502370549\n",
      "Val loss: 0.8904871886970829, accuracy: 0.74488657493492\n",
      "\n",
      "\n",
      "Epoch: 73\n",
      "Train loss: 0.3572278038904305, accuracy: 0.8741749558427071\n",
      "Val loss: 0.7239143955703117, accuracy: 0.793603570100409\n",
      "\n",
      "\n",
      "Epoch: 74\n",
      "Train loss: 0.2971991844017443, accuracy: 0.894673236032351\n",
      "Val loss: 0.7380710065520001, accuracy: 0.7883971736705094\n",
      "\n",
      "\n",
      "Epoch: 75\n",
      "Train loss: 0.3228429045371986, accuracy: 0.8848656688667844\n",
      "Val loss: 0.842143934337833, accuracy: 0.7556712532539978\n",
      "\n",
      "\n",
      "Epoch: 76\n",
      "Train loss: 0.275589229011775, accuracy: 0.9017384029004369\n",
      "Val loss: 0.7831504797571962, accuracy: 0.7703607288955002\n",
      "\n",
      "\n",
      "Epoch: 77\n",
      "Train loss: 0.3016778827544713, accuracy: 0.895416937807939\n",
      "Val loss: 0.7101836566501233, accuracy: 0.797694310152473\n",
      "\n",
      "\n",
      "Epoch: 78\n",
      "Train loss: 0.28644926586985153, accuracy: 0.8977874872176257\n",
      "Val loss: 0.7576898447241026, accuracy: 0.7872815172926738\n",
      "\n",
      "\n",
      "Epoch: 79\n",
      "Train loss: 0.2818649397529035, accuracy: 0.899879148461467\n",
      "Val loss: 0.8066610847320996, accuracy: 0.770918557084418\n",
      "\n",
      "\n",
      "Epoch: 80\n",
      "Train loss: 0.316700283991722, accuracy: 0.8882123268569304\n",
      "Val loss: 0.7474888266795362, accuracy: 0.7898847155076236\n",
      "\n",
      "\n",
      "Epoch: 81\n",
      "Train loss: 0.2922352696507906, accuracy: 0.8958817514176816\n",
      "Val loss: 0.6759006738795716, accuracy: 0.80866493120119\n",
      "\n",
      "\n",
      "Epoch: 82\n",
      "Train loss: 0.2569766386898741, accuracy: 0.9091754206563168\n",
      "Val loss: 0.6593683176698539, accuracy: 0.8153588694682038\n",
      "\n",
      "\n",
      "Epoch: 83\n",
      "Train loss: 0.28444537235046613, accuracy: 0.9015524774565399\n",
      "Val loss: 0.8110619046821963, accuracy: 0.7783562662699889\n",
      "\n",
      "\n",
      "Epoch: 84\n",
      "Train loss: 0.3294133522663017, accuracy: 0.8861671469740634\n",
      "Val loss: 0.7210568637376206, accuracy: 0.8110821866865006\n",
      "\n",
      "\n",
      "Epoch: 85\n",
      "Train loss: 0.261670271176947, accuracy: 0.9088035697685228\n",
      "Val loss: 0.6916222931860814, accuracy: 0.8110821866865006\n",
      "\n",
      "\n",
      "Epoch: 86\n",
      "Train loss: 0.22704683066500467, accuracy: 0.9203774286511109\n",
      "Val loss: 0.7597772080510822, accuracy: 0.7919300855336556\n",
      "\n",
      "\n",
      "Epoch: 87\n",
      "Train loss: 0.23956831229977574, accuracy: 0.9171237333829134\n",
      "Val loss: 0.769762171627022, accuracy: 0.791558200074377\n",
      "\n",
      "\n",
      "Epoch: 88\n",
      "Train loss: 0.24413459230845685, accuracy: 0.9145672585293297\n",
      "Val loss: 0.7241131184977996, accuracy: 0.8056898475269617\n",
      "\n",
      "\n",
      "Epoch: 89\n",
      "Train loss: 0.26500581898210085, accuracy: 0.9068048712466301\n",
      "Val loss: 0.8003016120884755, accuracy: 0.7779843808107103\n",
      "\n",
      "\n",
      "Epoch: 90\n",
      "Train loss: 0.27198056820002564, accuracy: 0.9037835827833038\n",
      "Val loss: 0.7432098331926658, accuracy: 0.7963927110449981\n",
      "\n",
      "\n",
      "Epoch: 91\n",
      "Train loss: 0.2199994482559221, accuracy: 0.9226550153388491\n",
      "Val loss: 0.7956309342925191, accuracy: 0.7906284864261808\n",
      "\n",
      "\n",
      "Epoch: 92\n",
      "Train loss: 0.3635901485559299, accuracy: 0.8731058845402994\n",
      "Val loss: 0.7927933464089305, accuracy: 0.7857939754555597\n",
      "\n",
      "\n",
      "Epoch: 93\n",
      "Train loss: 0.32843596755798904, accuracy: 0.8820767872083295\n",
      "Val loss: 0.7774818540062768, accuracy: 0.7889550018594274\n",
      "\n",
      "\n",
      "Epoch: 94\n",
      "Train loss: 0.26534385369110364, accuracy: 0.9070837594124755\n",
      "Val loss: 0.6672216987645329, accuracy: 0.8214949795462997\n",
      "\n",
      "\n",
      "Epoch: 95\n",
      "Train loss: 0.26735616950970154, accuracy: 0.9053639490564284\n",
      "Val loss: 0.82775372439665, accuracy: 0.7781703235403495\n",
      "\n",
      "\n",
      "Epoch: 96\n",
      "Train loss: 0.23897035973349323, accuracy: 0.9155898484707632\n",
      "Val loss: 0.7529023268401024, accuracy: 0.7986240238006694\n",
      "\n",
      "\n",
      "Epoch: 97\n",
      "Train loss: 0.2252323952729212, accuracy: 0.9217253881193641\n",
      "Val loss: 0.8018518730183435, accuracy: 0.7850502045370026\n",
      "\n",
      "\n",
      "Epoch: 98\n",
      "Train loss: 0.21471476060473987, accuracy: 0.9235846425583341\n",
      "Val loss: 0.8022330195409917, accuracy: 0.7954629973968018\n",
      "\n",
      "\n",
      "Epoch: 99\n",
      "Train loss: 0.2935129024120936, accuracy: 0.8958817514176816\n",
      "Val loss: 0.8135055677294066, accuracy: 0.7725920416511715\n",
      "\n",
      "\n",
      "Epoch: 100\n",
      "Train loss: 0.23792439413949482, accuracy: 0.9165659570512225\n",
      "Val loss: 0.7984756162814732, accuracy: 0.7902566009669022\n",
      "\n",
      "\n",
      "Epoch: 101\n",
      "Train loss: 0.25090718897975733, accuracy: 0.9123826345635401\n",
      "Val loss: 0.7561730779028595, accuracy: 0.7995537374488657\n",
      "\n",
      "\n",
      "Epoch: 102\n",
      "Train loss: 0.28311999732703097, accuracy: 0.9025750673979734\n",
      "Val loss: 0.8928649756889797, accuracy: 0.76273707698029\n",
      "\n",
      "\n",
      "Epoch: 103\n",
      "Train loss: 0.2860208441483173, accuracy: 0.898763595798085\n",
      "Val loss: 0.769017569102865, accuracy: 0.7969505392339159\n",
      "\n",
      "\n",
      "Epoch: 104\n",
      "Train loss: 0.22299384468354466, accuracy: 0.9221437203681324\n",
      "Val loss: 0.6979688977875999, accuracy: 0.8201933804388248\n",
      "\n",
      "\n",
      "Epoch: 105\n",
      "Train loss: 0.21729779269511984, accuracy: 0.9216789067583899\n",
      "Val loss: 0.7104790607497079, accuracy: 0.8190777240609892\n",
      "\n",
      "\n",
      "Epoch: 106\n",
      "Train loss: 0.18337645524993443, accuracy: 0.9355303523287162\n",
      "Val loss: 0.7256921563905693, accuracy: 0.8045741911491261\n",
      "\n",
      "\n",
      "Epoch: 107\n",
      "Train loss: 0.2596760095308635, accuracy: 0.9091289392953426\n",
      "Val loss: 0.9451845382260939, accuracy: 0.7671997024916326\n",
      "\n",
      "\n",
      "Epoch: 108\n",
      "Train loss: 0.27111500305203884, accuracy: 0.9080133866319606\n",
      "Val loss: 0.8902984224052046, accuracy: 0.761993306061733\n",
      "\n",
      "\n",
      "Epoch: 109\n",
      "Train loss: 0.256641197040576, accuracy: 0.9108487496513898\n",
      "Val loss: 0.7261142166807288, accuracy: 0.8138713276310896\n",
      "\n",
      "\n",
      "Epoch: 110\n",
      "Train loss: 0.20996942028210733, accuracy: 0.9259087106070466\n",
      "Val loss: 0.7140399575588117, accuracy: 0.8084789884715508\n",
      "\n",
      "\n",
      "Epoch: 111\n",
      "Train loss: 0.18447667101402093, accuracy: 0.9342288742214372\n",
      "Val loss: 0.7097833771916648, accuracy: 0.8161026403867608\n",
      "\n",
      "\n",
      "Epoch: 112\n",
      "Train loss: 0.19412970338721497, accuracy: 0.9329738774751325\n",
      "Val loss: 0.7705542488185035, accuracy: 0.7969505392339159\n",
      "\n",
      "\n",
      "Epoch: 113\n",
      "Train loss: 0.2603069291251559, accuracy: 0.9096402342660593\n",
      "Val loss: 0.6724711389690582, accuracy: 0.81889178133135\n",
      "\n",
      "\n",
      "Epoch: 114\n",
      "Train loss: 0.20042884955039816, accuracy: 0.929162405875244\n",
      "Val loss: 0.8041822408024996, accuracy: 0.7989959092599479\n",
      "\n",
      "\n",
      "Epoch: 115\n",
      "Train loss: 0.15826251434051958, accuracy: 0.9448731058845403\n",
      "Val loss: 0.7862727145760332, accuracy: 0.8066195611751581\n",
      "\n",
      "\n",
      "Epoch: 116\n",
      "Train loss: 0.1910180086771263, accuracy: 0.9331133215580553\n",
      "Val loss: 0.7587316110924125, accuracy: 0.8056898475269617\n",
      "\n",
      "\n",
      "Epoch: 117\n",
      "Train loss: 0.2602843038707991, accuracy: 0.910151529236776\n",
      "Val loss: 0.7204993863494124, accuracy: 0.8114540721457791\n",
      "\n",
      "\n",
      "Epoch: 118\n",
      "Train loss: 0.16552090146491838, accuracy: 0.9413870038114716\n",
      "Val loss: 0.6838025577496578, accuracy: 0.8322796578653775\n",
      "\n",
      "\n",
      "Epoch: 119\n",
      "Train loss: 0.14530030932691979, accuracy: 0.9495212419819652\n",
      "Val loss: 0.7288319557384294, accuracy: 0.8278170323540349\n",
      "\n",
      "\n",
      "Epoch: 120\n",
      "Train loss: 0.1616526835211826, accuracy: 0.9432462582504416\n",
      "Val loss: 0.761944039730655, accuracy: 0.8121978430643362\n",
      "\n",
      "\n",
      "Epoch: 121\n",
      "Train loss: 0.18445622189299618, accuracy: 0.9356233150506647\n",
      "Val loss: 0.9305246669193677, accuracy: 0.7724060989215321\n",
      "\n",
      "\n",
      "Epoch: 122\n",
      "Train loss: 0.2896493843239657, accuracy: 0.8998326671004927\n",
      "Val loss: 0.7428394593949564, accuracy: 0.8055039047973225\n",
      "\n",
      "\n",
      "Epoch: 123\n",
      "Train loss: 0.3168137298883576, accuracy: 0.895788788695733\n",
      "Val loss: 0.7307677491086263, accuracy: 0.8157307549274824\n",
      "\n",
      "\n",
      "Epoch: 124\n",
      "Train loss: 0.13606847196532035, accuracy: 0.9528214186111369\n",
      "Val loss: 0.6951806531514586, accuracy: 0.8343250278914095\n",
      "\n",
      "\n",
      "Epoch: 125\n",
      "Train loss: 0.18101326119581695, accuracy: 0.9383192339871711\n",
      "Val loss: 0.7366407586988263, accuracy: 0.8114540721457791\n",
      "\n",
      "\n",
      "Epoch: 126\n",
      "Train loss: 0.12934936936489322, accuracy: 0.9548665984940039\n",
      "Val loss: 0.6376934215986139, accuracy: 0.843622164373373\n",
      "\n",
      "\n",
      "Epoch: 127\n",
      "Train loss: 0.1014998422614746, accuracy: 0.9649995351863903\n",
      "Val loss: 0.7739145619889625, accuracy: 0.8114540721457791\n",
      "\n",
      "\n",
      "Epoch: 128\n",
      "Train loss: 0.19633466850883446, accuracy: 0.931021660314214\n",
      "Val loss: 0.8067104768225498, accuracy: 0.797694310152473\n",
      "\n",
      "\n",
      "Epoch: 129\n",
      "Train loss: 0.2545571806396019, accuracy: 0.9108022682904156\n",
      "Val loss: 0.7878100490339541, accuracy: 0.8019709929341763\n",
      "\n",
      "\n",
      "Epoch: 130\n",
      "Train loss: 0.2126933376582635, accuracy: 0.927117225992377\n",
      "Val loss: 0.8183464374671686, accuracy: 0.7930457419114912\n",
      "\n",
      "\n",
      "Epoch: 131\n",
      "Train loss: 0.18681181184729315, accuracy: 0.9355768336896905\n",
      "Val loss: 0.691132952618129, accuracy: 0.8317218296764597\n",
      "\n",
      "\n",
      "Epoch: 132\n",
      "Train loss: 0.14075917348388936, accuracy: 0.9501254996746304\n",
      "Val loss: 0.817585015859955, accuracy: 0.8034585347712905\n",
      "\n",
      "\n",
      "Epoch: 133\n",
      "Train loss: 0.1537015003498418, accuracy: 0.9455238449381798\n",
      "Val loss: 0.7708916817956274, accuracy: 0.8172182967645965\n",
      "\n",
      "\n",
      "Epoch: 134\n",
      "Train loss: 0.1941820873077268, accuracy: 0.9323696197824672\n",
      "Val loss: 0.7983126980347809, accuracy: 0.7991818519895872\n",
      "\n",
      "\n",
      "Epoch: 135\n",
      "Train loss: 0.2793151495225525, accuracy: 0.9074091289392954\n",
      "Val loss: 0.6722495895240688, accuracy: 0.8287467460022313\n",
      "\n",
      "\n",
      "Epoch: 136\n",
      "Train loss: 0.16417301279207228, accuracy: 0.9433857023333643\n",
      "Val loss: 1.0501488783227417, accuracy: 0.7430271476385273\n",
      "\n",
      "\n",
      "Epoch: 137\n",
      "Train loss: 0.20922982612927962, accuracy: 0.9263270428558148\n",
      "Val loss: 0.7624777433773805, accuracy: 0.8146150985496468\n",
      "\n",
      "\n",
      "Epoch: 138\n",
      "Train loss: 0.1110094451712855, accuracy: 0.9613739890303988\n",
      "Val loss: 0.7087208677752393, accuracy: 0.8378579397545556\n",
      "\n",
      "\n",
      "Epoch: 139\n",
      "Train loss: 0.1109348615591462, accuracy: 0.96346565027424\n",
      "Val loss: 0.7021857633072898, accuracy: 0.83674228337672\n",
      "\n",
      "\n",
      "Epoch: 140\n",
      "Train loss: 0.17303235967124694, accuracy: 0.9402249697871153\n",
      "Val loss: 0.8815517015464246, accuracy: 0.7963927110449981\n",
      "\n",
      "\n",
      "Epoch: 141\n",
      "Train loss: 0.31700876516137544, accuracy: 0.8921167611787673\n",
      "Val loss: 0.922142348044776, accuracy: 0.7675715879509111\n",
      "\n",
      "\n",
      "Epoch: 142\n",
      "Train loss: 0.1984181498338645, accuracy: 0.931021660314214\n",
      "Val loss: 0.7083953405100424, accuracy: 0.8205652658981034\n",
      "\n",
      "\n",
      "Epoch: 143\n",
      "Train loss: 0.12314434557463297, accuracy: 0.9570977038207679\n",
      "Val loss: 0.723669544654781, accuracy: 0.8332093715135738\n",
      "\n",
      "\n",
      "Epoch: 144\n",
      "Train loss: 0.13950323536431045, accuracy: 0.9518917913916519\n",
      "Val loss: 0.7253218422418369, accuracy: 0.8296764596504277\n",
      "\n",
      "\n",
      "Epoch: 145\n",
      "Train loss: 0.13064385517604668, accuracy: 0.9543553035232871\n",
      "Val loss: 0.8044290534179421, accuracy: 0.8172182967645965\n",
      "\n",
      "\n",
      "Epoch: 146\n",
      "Train loss: 0.159050592817238, accuracy: 0.9450125499674631\n",
      "Val loss: 0.9792049175835578, accuracy: 0.7789140944589067\n",
      "\n",
      "\n",
      "Epoch: 147\n",
      "Train loss: 0.2873063700215972, accuracy: 0.9014595147345914\n",
      "Val loss: 0.7902941864278956, accuracy: 0.7917441428040164\n",
      "\n",
      "\n",
      "Epoch: 148\n",
      "Train loss: 0.19352218287010733, accuracy: 0.9333922097239007\n",
      "Val loss: 0.7338244571655645, accuracy: 0.8242841204908888\n",
      "\n",
      "\n",
      "Epoch: 149\n",
      "Train loss: 0.21050945725307552, accuracy: 0.9298131449288836\n",
      "Val loss: 0.7282409062169218, accuracy: 0.821680922275939\n",
      "\n",
      "\n",
      "Epoch: 150\n",
      "Train loss: 0.1511788651093659, accuracy: 0.9483127265966348\n",
      "Val loss: 0.7916951346415133, accuracy: 0.8198214949795463\n",
      "\n",
      "\n",
      "Epoch: 151\n",
      "Train loss: 0.11745705822148686, accuracy: 0.9587245514548666\n",
      "Val loss: 0.7245411807473773, accuracy: 0.8270732614354779\n",
      "\n",
      "\n",
      "Epoch: 152\n",
      "Train loss: 0.1307794433036895, accuracy: 0.9564004834061541\n",
      "Val loss: 0.8388646203108816, accuracy: 0.7986240238006694\n",
      "\n",
      "\n",
      "Epoch: 153\n",
      "Train loss: 0.13232648616731638, accuracy: 0.9552849307427721\n",
      "Val loss: 0.7114910351471991, accuracy: 0.8400892525102268\n",
      "\n",
      "\n",
      "Epoch: 154\n",
      "Train loss: 0.1272531034038935, accuracy: 0.9555638189086176\n",
      "Val loss: 0.9179300442864434, accuracy: 0.7878393454815916\n",
      "\n",
      "\n",
      "Epoch: 155\n",
      "Train loss: 0.1709797469778022, accuracy: 0.9398066375383471\n",
      "Val loss: 0.7667795174092598, accuracy: 0.8244700632205281\n",
      "\n",
      "\n",
      "Epoch: 156\n",
      "Train loss: 0.11244759010198646, accuracy: 0.9610486195035791\n",
      "Val loss: 0.7577511492649743, accuracy: 0.8293045741911491\n",
      "\n",
      "\n",
      "Epoch: 157\n",
      "Train loss: 0.1262041405681408, accuracy: 0.9571441851817422\n",
      "Val loss: 0.8029907309641046, accuracy: 0.8142432130903682\n",
      "\n",
      "\n",
      "Epoch: 158\n",
      "Train loss: 0.15776417325038256, accuracy: 0.9455238449381798\n",
      "Val loss: 0.7433538433564734, accuracy: 0.8311640014875419\n",
      "\n",
      "\n",
      "Epoch: 159\n",
      "Train loss: 0.13289431528059026, accuracy: 0.953007344055034\n",
      "Val loss: 0.8203603840308723, accuracy: 0.8218668650055783\n",
      "\n",
      "\n",
      "Epoch: 160\n",
      "Train loss: 0.17347746740888462, accuracy: 0.9401320070651669\n",
      "Val loss: 1.0948124532807755, accuracy: 0.7536258832279658\n",
      "\n",
      "\n",
      "Epoch: 161\n",
      "Train loss: 0.22517163082687144, accuracy: 0.926187598772892\n",
      "Val loss: 0.7536977187502824, accuracy: 0.8114540721457791\n",
      "\n",
      "\n",
      "Epoch: 162\n",
      "Train loss: 0.12630626980097218, accuracy: 0.9562145579622572\n",
      "Val loss: 0.7550069098359105, accuracy: 0.8270732614354779\n",
      "\n",
      "\n",
      "Epoch: 163\n",
      "Train loss: 0.18558875974130928, accuracy: 0.9350190573579994\n",
      "Val loss: 0.8148658446726545, accuracy: 0.810710301227222\n",
      "\n",
      "\n",
      "Epoch: 164\n",
      "Train loss: 0.11750159987571504, accuracy: 0.9588639955377893\n",
      "Val loss: 0.7394850688736072, accuracy: 0.839531424321309\n",
      "\n",
      "\n",
      "Epoch: 165\n",
      "Train loss: 0.07866083760224189, accuracy: 0.9735056242446779\n",
      "Val loss: 0.7506370694719756, accuracy: 0.8419486798066196\n",
      "\n",
      "\n",
      "Epoch: 166\n",
      "Train loss: 0.0927038705313229, accuracy: 0.967741935483871\n",
      "Val loss: 0.7613040963572257, accuracy: 0.8322796578653775\n",
      "\n",
      "\n",
      "Epoch: 167\n",
      "Train loss: 0.16774819315274964, accuracy: 0.9425025564748536\n",
      "Val loss: 0.9839745231080826, accuracy: 0.7657121606545184\n",
      "\n",
      "\n",
      "Epoch: 168\n",
      "Train loss: 0.30343594265419654, accuracy: 0.9003439620712095\n",
      "Val loss: 0.7246404521613371, accuracy: 0.8272592041651171\n",
      "\n",
      "\n",
      "Epoch: 169\n",
      "Train loss: 0.0823205945788401, accuracy: 0.9731337733568839\n",
      "Val loss: 0.7086994154673165, accuracy: 0.8458534771290442\n",
      "\n",
      "\n",
      "Epoch: 170\n",
      "Train loss: 0.16716205641562204, accuracy: 0.9425490378358279\n",
      "Val loss: 0.8619689290831195, accuracy: 0.795648940126441\n",
      "\n",
      "\n",
      "Epoch: 171\n",
      "Train loss: 0.12541656178801744, accuracy: 0.9567258529329739\n",
      "Val loss: 0.6857840053961596, accuracy: 0.8445518780215694\n",
      "\n",
      "\n",
      "Epoch: 172\n",
      "Train loss: 0.11227726265982013, accuracy: 0.9613739890303988\n",
      "Val loss: 0.6850358102790659, accuracy: 0.847712904425437\n",
      "\n",
      "\n",
      "Epoch: 173\n",
      "Train loss: 0.07288930713548596, accuracy: 0.9752719159616994\n",
      "Val loss: 0.8771101925709115, accuracy: 0.8079211602826329\n",
      "\n",
      "\n",
      "Epoch: 174\n",
      "Train loss: 0.2866792504942261, accuracy: 0.9081993120758576\n",
      "Val loss: 1.008291839311982, accuracy: 0.7447006322052808\n",
      "\n",
      "\n",
      "Epoch: 175\n",
      "Train loss: 0.2030867925499899, accuracy: 0.93250906386539\n",
      "Val loss: 0.6722301136343486, accuracy: 0.8460394198586835\n",
      "\n",
      "\n",
      "Epoch: 176\n",
      "Train loss: 0.07552487356263576, accuracy: 0.9739239564934461\n",
      "Val loss: 0.7253594470760023, accuracy: 0.8341390851617702\n",
      "\n",
      "\n",
      "Epoch: 177\n",
      "Train loss: 0.18793846416913346, accuracy: 0.9386446035139909\n",
      "Val loss: 1.027048691181766, accuracy: 0.7500929713648197\n",
      "\n",
      "\n",
      "Epoch: 178\n",
      "Train loss: 0.1718253573854528, accuracy: 0.943153295528493\n",
      "Val loss: 0.7153225114195708, accuracy: 0.8319077724060989\n",
      "\n",
      "\n",
      "Epoch: 179\n",
      "Train loss: 0.07440163772125509, accuracy: 0.9753183973226736\n",
      "Val loss: 0.8169332158392626, accuracy: 0.813499442171811\n",
      "\n",
      "\n",
      "Epoch: 180\n",
      "Train loss: 0.14227832983592256, accuracy: 0.9508692014502185\n",
      "Val loss: 1.0047796215652842, accuracy: 0.764038676087765\n",
      "\n",
      "\n",
      "Epoch: 181\n",
      "Train loss: 0.23846764431446804, accuracy: 0.9187970623779864\n",
      "Val loss: 0.7600969951717593, accuracy: 0.8222387504648568\n",
      "\n",
      "\n",
      "Epoch: 182\n",
      "Train loss: 0.1092197853028414, accuracy: 0.9630008366644975\n",
      "Val loss: 0.6769829397309709, accuracy: 0.8523614726664187\n",
      "\n",
      "\n",
      "Epoch: 183\n",
      "Train loss: 0.059787374528683705, accuracy: 0.9795946825323045\n",
      "Val loss: 0.7016535223350511, accuracy: 0.8452956489401264\n",
      "\n",
      "\n",
      "Epoch: 184\n",
      "Train loss: 0.07650540966013833, accuracy: 0.9741563632983173\n",
      "Val loss: 0.7481331930988648, accuracy: 0.8358125697285236\n",
      "\n",
      "\n",
      "Epoch: 185\n",
      "Train loss: 0.13719552505040708, accuracy: 0.9523101236404202\n",
      "Val loss: 0.7557227054285977, accuracy: 0.8278170323540349\n",
      "\n",
      "\n",
      "Epoch: 186\n",
      "Train loss: 0.12790540424315383, accuracy: 0.9547271544110811\n",
      "Val loss: 0.7707554129873256, accuracy: 0.8239122350316103\n",
      "\n",
      "\n",
      "Epoch: 187\n",
      "Train loss: 0.10281884250612637, accuracy: 0.9641163893278796\n",
      "Val loss: 0.7403759155720306, accuracy: 0.8345109706210487\n",
      "\n",
      "\n",
      "Epoch: 188\n",
      "Train loss: 0.06754435690623228, accuracy: 0.9779213535372315\n",
      "Val loss: 0.728750168124554, accuracy: 0.8497582744514689\n",
      "\n",
      "\n",
      "Epoch: 189\n",
      "Train loss: 0.12629965520640937, accuracy: 0.9576089987914846\n",
      "Val loss: 0.804258145976572, accuracy: 0.817590182223875\n",
      "\n",
      "\n",
      "Epoch: 190\n",
      "Train loss: 0.19505920692720055, accuracy: 0.9340894301385144\n",
      "Val loss: 0.8700055497514576, accuracy: 0.7993677947192265\n",
      "\n",
      "\n",
      "Epoch: 191\n",
      "Train loss: 0.1965470556916053, accuracy: 0.932323138421493\n",
      "Val loss: 0.8429934298650799, accuracy: 0.8036444775009297\n",
      "\n",
      "\n",
      "Epoch: 192\n",
      "Train loss: 0.16341051842750332, accuracy: 0.944454773635772\n",
      "Val loss: 0.8285381315188055, accuracy: 0.80866493120119\n",
      "\n",
      "\n",
      "Epoch: 193\n",
      "Train loss: 0.1863786478218585, accuracy: 0.9379473830993772\n",
      "Val loss: 0.7358036502757982, accuracy: 0.8365563406470807\n",
      "\n",
      "\n",
      "Epoch: 194\n",
      "Train loss: 0.11170723234395329, accuracy: 0.9643487961327508\n",
      "Val loss: 0.8434478162433543, accuracy: 0.801785050204537\n",
      "\n",
      "\n",
      "Epoch: 195\n",
      "Train loss: 0.16195525545645925, accuracy: 0.9439434786650553\n",
      "Val loss: 0.8644826000133116, accuracy: 0.8014131647452585\n",
      "\n",
      "\n",
      "Epoch: 196\n",
      "Train loss: 0.1534171309476228, accuracy: 0.9473366180161755\n",
      "Val loss: 0.7291028828991528, accuracy: 0.8352547415396058\n",
      "\n",
      "\n",
      "Epoch: 197\n",
      "Train loss: 0.1993855151230156, accuracy: 0.9430603328065446\n",
      "Val loss: 1.124898559677419, accuracy: 0.715321680922276\n",
      "\n",
      "\n",
      "Epoch: 198\n",
      "Train loss: 0.22029180190865078, accuracy: 0.9253974156363298\n",
      "Val loss: 0.7701707405643243, accuracy: 0.8172182967645965\n",
      "\n",
      "\n",
      "Epoch: 199\n",
      "Train loss: 0.10464166037193226, accuracy: 0.9645812029376221\n",
      "Val loss: 0.7486157521478036, accuracy: 0.8339531424321309\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([2.4962200932956224,\n",
       "  2.2537484609959924,\n",
       "  2.0359320370716776,\n",
       "  1.8935093892998132,\n",
       "  1.7963539639779977,\n",
       "  1.7157956381164166,\n",
       "  1.6597070675241112,\n",
       "  1.6064825199168333,\n",
       "  1.4819212535660629,\n",
       "  1.436675710140401,\n",
       "  1.4163622064773085,\n",
       "  1.3786301336135964,\n",
       "  1.3272860475972625,\n",
       "  1.3229098799637151,\n",
       "  1.2538860841922028,\n",
       "  1.225153247142266,\n",
       "  1.1687722867047399,\n",
       "  1.1579253379769117,\n",
       "  1.1613718626212595,\n",
       "  1.0687676070624594,\n",
       "  1.0597836052583498,\n",
       "  1.0394781961021697,\n",
       "  1.0113681283174,\n",
       "  0.9512455758911488,\n",
       "  0.9662307149151328,\n",
       "  0.9054056161053066,\n",
       "  0.8950533067425399,\n",
       "  0.8451942268762822,\n",
       "  0.8838818571187865,\n",
       "  0.7919543253795868,\n",
       "  0.7937432247998158,\n",
       "  0.7819710439040757,\n",
       "  0.7036213034633545,\n",
       "  0.701726884450502,\n",
       "  0.6880338927333524,\n",
       "  0.6880959391571769,\n",
       "  0.7035467872125815,\n",
       "  0.6476780633185226,\n",
       "  0.6199766868739098,\n",
       "  0.5937646318237788,\n",
       "  0.6153722383047975,\n",
       "  0.5703832501675589,\n",
       "  0.616331164512402,\n",
       "  0.5683505828522213,\n",
       "  0.534311089881836,\n",
       "  0.5337520186659727,\n",
       "  0.5215028152874082,\n",
       "  0.4892959909095766,\n",
       "  0.5282136544376055,\n",
       "  0.49300057684177245,\n",
       "  0.4702070959191501,\n",
       "  0.5347055805054165,\n",
       "  0.427522484095021,\n",
       "  0.4104117920925773,\n",
       "  0.44079722174151276,\n",
       "  0.48292121589543974,\n",
       "  0.3940741437412587,\n",
       "  0.36704946573842046,\n",
       "  0.4269925632657,\n",
       "  0.4194475779432983,\n",
       "  0.3706787081636704,\n",
       "  0.3964790462549927,\n",
       "  0.39281501252045914,\n",
       "  0.3872052518211245,\n",
       "  0.37693397522504596,\n",
       "  0.41833758605194765,\n",
       "  0.32914552750270076,\n",
       "  0.2967928742677004,\n",
       "  0.3429090168254021,\n",
       "  0.28500870745111334,\n",
       "  0.320230091311868,\n",
       "  0.3508410321781823,\n",
       "  0.2840117776167817,\n",
       "  0.3572278038904305,\n",
       "  0.2971991844017443,\n",
       "  0.3228429045371986,\n",
       "  0.275589229011775,\n",
       "  0.3016778827544713,\n",
       "  0.28644926586985153,\n",
       "  0.2818649397529035,\n",
       "  0.316700283991722,\n",
       "  0.2922352696507906,\n",
       "  0.2569766386898741,\n",
       "  0.28444537235046613,\n",
       "  0.3294133522663017,\n",
       "  0.261670271176947,\n",
       "  0.22704683066500467,\n",
       "  0.23956831229977574,\n",
       "  0.24413459230845685,\n",
       "  0.26500581898210085,\n",
       "  0.27198056820002564,\n",
       "  0.2199994482559221,\n",
       "  0.3635901485559299,\n",
       "  0.32843596755798904,\n",
       "  0.26534385369110364,\n",
       "  0.26735616950970154,\n",
       "  0.23897035973349323,\n",
       "  0.2252323952729212,\n",
       "  0.21471476060473987,\n",
       "  0.2935129024120936,\n",
       "  0.23792439413949482,\n",
       "  0.25090718897975733,\n",
       "  0.28311999732703097,\n",
       "  0.2860208441483173,\n",
       "  0.22299384468354466,\n",
       "  0.21729779269511984,\n",
       "  0.18337645524993443,\n",
       "  0.2596760095308635,\n",
       "  0.27111500305203884,\n",
       "  0.256641197040576,\n",
       "  0.20996942028210733,\n",
       "  0.18447667101402093,\n",
       "  0.19412970338721497,\n",
       "  0.2603069291251559,\n",
       "  0.20042884955039816,\n",
       "  0.15826251434051958,\n",
       "  0.1910180086771263,\n",
       "  0.2602843038707991,\n",
       "  0.16552090146491838,\n",
       "  0.14530030932691979,\n",
       "  0.1616526835211826,\n",
       "  0.18445622189299618,\n",
       "  0.2896493843239657,\n",
       "  0.3168137298883576,\n",
       "  0.13606847196532035,\n",
       "  0.18101326119581695,\n",
       "  0.12934936936489322,\n",
       "  0.1014998422614746,\n",
       "  0.19633466850883446,\n",
       "  0.2545571806396019,\n",
       "  0.2126933376582635,\n",
       "  0.18681181184729315,\n",
       "  0.14075917348388936,\n",
       "  0.1537015003498418,\n",
       "  0.1941820873077268,\n",
       "  0.2793151495225525,\n",
       "  0.16417301279207228,\n",
       "  0.20922982612927962,\n",
       "  0.1110094451712855,\n",
       "  0.1109348615591462,\n",
       "  0.17303235967124694,\n",
       "  0.31700876516137544,\n",
       "  0.1984181498338645,\n",
       "  0.12314434557463297,\n",
       "  0.13950323536431045,\n",
       "  0.13064385517604668,\n",
       "  0.159050592817238,\n",
       "  0.2873063700215972,\n",
       "  0.19352218287010733,\n",
       "  0.21050945725307552,\n",
       "  0.1511788651093659,\n",
       "  0.11745705822148686,\n",
       "  0.1307794433036895,\n",
       "  0.13232648616731638,\n",
       "  0.1272531034038935,\n",
       "  0.1709797469778022,\n",
       "  0.11244759010198646,\n",
       "  0.1262041405681408,\n",
       "  0.15776417325038256,\n",
       "  0.13289431528059026,\n",
       "  0.17347746740888462,\n",
       "  0.22517163082687144,\n",
       "  0.12630626980097218,\n",
       "  0.18558875974130928,\n",
       "  0.11750159987571504,\n",
       "  0.07866083760224189,\n",
       "  0.0927038705313229,\n",
       "  0.16774819315274964,\n",
       "  0.30343594265419654,\n",
       "  0.0823205945788401,\n",
       "  0.16716205641562204,\n",
       "  0.12541656178801744,\n",
       "  0.11227726265982013,\n",
       "  0.07288930713548596,\n",
       "  0.2866792504942261,\n",
       "  0.2030867925499899,\n",
       "  0.07552487356263576,\n",
       "  0.18793846416913346,\n",
       "  0.1718253573854528,\n",
       "  0.07440163772125509,\n",
       "  0.14227832983592256,\n",
       "  0.23846764431446804,\n",
       "  0.1092197853028414,\n",
       "  0.059787374528683705,\n",
       "  0.07650540966013833,\n",
       "  0.13719552505040708,\n",
       "  0.12790540424315383,\n",
       "  0.10281884250612637,\n",
       "  0.06754435690623228,\n",
       "  0.12629965520640937,\n",
       "  0.19505920692720055,\n",
       "  0.1965470556916053,\n",
       "  0.16341051842750332,\n",
       "  0.1863786478218585,\n",
       "  0.11170723234395329,\n",
       "  0.16195525545645925,\n",
       "  0.1534171309476228,\n",
       "  0.1993855151230156,\n",
       "  0.22029180190865078,\n",
       "  0.10464166037193226],\n",
       " [0.18625081342381705,\n",
       "  0.2518824951194571,\n",
       "  0.31853676675653064,\n",
       "  0.3606488797992005,\n",
       "  0.3947661987542995,\n",
       "  0.41902946918285766,\n",
       "  0.4337640606116947,\n",
       "  0.44952124198196525,\n",
       "  0.4926559449660686,\n",
       "  0.5023240680487124,\n",
       "  0.5105047875801804,\n",
       "  0.5255647485358371,\n",
       "  0.5442502556474853,\n",
       "  0.5422050757646184,\n",
       "  0.5673514920516872,\n",
       "  0.5750673979734127,\n",
       "  0.5988658547922283,\n",
       "  0.6002602956214558,\n",
       "  0.6003067769824301,\n",
       "  0.6340057636887608,\n",
       "  0.6348424281862973,\n",
       "  0.6400483406154133,\n",
       "  0.6501347959468253,\n",
       "  0.6688667844194478,\n",
       "  0.6611973598586967,\n",
       "  0.6844845217067955,\n",
       "  0.688667844194478,\n",
       "  0.7072139072232035,\n",
       "  0.6938272752626197,\n",
       "  0.7211583155154783,\n",
       "  0.7213442409593753,\n",
       "  0.724830343032444,\n",
       "  0.7564376684949335,\n",
       "  0.756158780329088,\n",
       "  0.7599237705680022,\n",
       "  0.7554615599144743,\n",
       "  0.7508134238170494,\n",
       "  0.7714046667286418,\n",
       "  0.7825601933624616,\n",
       "  0.7915310960304918,\n",
       "  0.7833503764990238,\n",
       "  0.7976666356790927,\n",
       "  0.7814446406990797,\n",
       "  0.7981779306498095,\n",
       "  0.8093799386446036,\n",
       "  0.810402528586037,\n",
       "  0.8151901087663846,\n",
       "  0.8243004555173375,\n",
       "  0.811332155805522,\n",
       "  0.8254624895416938,\n",
       "  0.8328065445756252,\n",
       "  0.8145858510737194,\n",
       "  0.8479594682532304,\n",
       "  0.8542809333457284,\n",
       "  0.845635400204518,\n",
       "  0.8282513712001487,\n",
       "  0.860555917077252,\n",
       "  0.8665984940039044,\n",
       "  0.8472622478386167,\n",
       "  0.8509342753555824,\n",
       "  0.8699451519940504,\n",
       "  0.8587431439992563,\n",
       "  0.8583712931114623,\n",
       "  0.8653899786185739,\n",
       "  0.8655294227014967,\n",
       "  0.8534442688481919,\n",
       "  0.8858417774472437,\n",
       "  0.8928139815933811,\n",
       "  0.8784047596913638,\n",
       "  0.8999721111834155,\n",
       "  0.8889560286325183,\n",
       "  0.8758018034768058,\n",
       "  0.9006228502370549,\n",
       "  0.8741749558427071,\n",
       "  0.894673236032351,\n",
       "  0.8848656688667844,\n",
       "  0.9017384029004369,\n",
       "  0.895416937807939,\n",
       "  0.8977874872176257,\n",
       "  0.899879148461467,\n",
       "  0.8882123268569304,\n",
       "  0.8958817514176816,\n",
       "  0.9091754206563168,\n",
       "  0.9015524774565399,\n",
       "  0.8861671469740634,\n",
       "  0.9088035697685228,\n",
       "  0.9203774286511109,\n",
       "  0.9171237333829134,\n",
       "  0.9145672585293297,\n",
       "  0.9068048712466301,\n",
       "  0.9037835827833038,\n",
       "  0.9226550153388491,\n",
       "  0.8731058845402994,\n",
       "  0.8820767872083295,\n",
       "  0.9070837594124755,\n",
       "  0.9053639490564284,\n",
       "  0.9155898484707632,\n",
       "  0.9217253881193641,\n",
       "  0.9235846425583341,\n",
       "  0.8958817514176816,\n",
       "  0.9165659570512225,\n",
       "  0.9123826345635401,\n",
       "  0.9025750673979734,\n",
       "  0.898763595798085,\n",
       "  0.9221437203681324,\n",
       "  0.9216789067583899,\n",
       "  0.9355303523287162,\n",
       "  0.9091289392953426,\n",
       "  0.9080133866319606,\n",
       "  0.9108487496513898,\n",
       "  0.9259087106070466,\n",
       "  0.9342288742214372,\n",
       "  0.9329738774751325,\n",
       "  0.9096402342660593,\n",
       "  0.929162405875244,\n",
       "  0.9448731058845403,\n",
       "  0.9331133215580553,\n",
       "  0.910151529236776,\n",
       "  0.9413870038114716,\n",
       "  0.9495212419819652,\n",
       "  0.9432462582504416,\n",
       "  0.9356233150506647,\n",
       "  0.8998326671004927,\n",
       "  0.895788788695733,\n",
       "  0.9528214186111369,\n",
       "  0.9383192339871711,\n",
       "  0.9548665984940039,\n",
       "  0.9649995351863903,\n",
       "  0.931021660314214,\n",
       "  0.9108022682904156,\n",
       "  0.927117225992377,\n",
       "  0.9355768336896905,\n",
       "  0.9501254996746304,\n",
       "  0.9455238449381798,\n",
       "  0.9323696197824672,\n",
       "  0.9074091289392954,\n",
       "  0.9433857023333643,\n",
       "  0.9263270428558148,\n",
       "  0.9613739890303988,\n",
       "  0.96346565027424,\n",
       "  0.9402249697871153,\n",
       "  0.8921167611787673,\n",
       "  0.931021660314214,\n",
       "  0.9570977038207679,\n",
       "  0.9518917913916519,\n",
       "  0.9543553035232871,\n",
       "  0.9450125499674631,\n",
       "  0.9014595147345914,\n",
       "  0.9333922097239007,\n",
       "  0.9298131449288836,\n",
       "  0.9483127265966348,\n",
       "  0.9587245514548666,\n",
       "  0.9564004834061541,\n",
       "  0.9552849307427721,\n",
       "  0.9555638189086176,\n",
       "  0.9398066375383471,\n",
       "  0.9610486195035791,\n",
       "  0.9571441851817422,\n",
       "  0.9455238449381798,\n",
       "  0.953007344055034,\n",
       "  0.9401320070651669,\n",
       "  0.926187598772892,\n",
       "  0.9562145579622572,\n",
       "  0.9350190573579994,\n",
       "  0.9588639955377893,\n",
       "  0.9735056242446779,\n",
       "  0.967741935483871,\n",
       "  0.9425025564748536,\n",
       "  0.9003439620712095,\n",
       "  0.9731337733568839,\n",
       "  0.9425490378358279,\n",
       "  0.9567258529329739,\n",
       "  0.9613739890303988,\n",
       "  0.9752719159616994,\n",
       "  0.9081993120758576,\n",
       "  0.93250906386539,\n",
       "  0.9739239564934461,\n",
       "  0.9386446035139909,\n",
       "  0.943153295528493,\n",
       "  0.9753183973226736,\n",
       "  0.9508692014502185,\n",
       "  0.9187970623779864,\n",
       "  0.9630008366644975,\n",
       "  0.9795946825323045,\n",
       "  0.9741563632983173,\n",
       "  0.9523101236404202,\n",
       "  0.9547271544110811,\n",
       "  0.9641163893278796,\n",
       "  0.9779213535372315,\n",
       "  0.9576089987914846,\n",
       "  0.9340894301385144,\n",
       "  0.932323138421493,\n",
       "  0.944454773635772,\n",
       "  0.9379473830993772,\n",
       "  0.9643487961327508,\n",
       "  0.9439434786650553,\n",
       "  0.9473366180161755,\n",
       "  0.9430603328065446,\n",
       "  0.9253974156363298,\n",
       "  0.9645812029376221],\n",
       " [2.3332459142811905,\n",
       "  2.0972689812668373,\n",
       "  1.975506011151434,\n",
       "  1.9348434735603517,\n",
       "  1.7261083431516984,\n",
       "  1.8964715888394703,\n",
       "  1.6359446550577093,\n",
       "  1.5137257493087553,\n",
       "  1.5538834108212216,\n",
       "  1.4802379283642582,\n",
       "  1.4983202117045336,\n",
       "  1.4393153740775413,\n",
       "  1.396672492091121,\n",
       "  1.3736190326863746,\n",
       "  1.2929406557529999,\n",
       "  1.303152663994119,\n",
       "  1.2847589817841552,\n",
       "  1.1810155253997299,\n",
       "  1.2371304628351623,\n",
       "  1.210337512153549,\n",
       "  1.1836432993035193,\n",
       "  1.1153916486269837,\n",
       "  1.089940560666457,\n",
       "  1.105512859476415,\n",
       "  1.0977378812542011,\n",
       "  1.0063707936913961,\n",
       "  1.0016320885757213,\n",
       "  1.1770218817351608,\n",
       "  0.9233048885406815,\n",
       "  0.974091748780768,\n",
       "  0.9565241618996971,\n",
       "  0.8975221924996367,\n",
       "  0.8980292035505735,\n",
       "  0.9403900694297388,\n",
       "  0.9706586692004276,\n",
       "  0.9523170798983703,\n",
       "  0.892493326187843,\n",
       "  0.9300027001564634,\n",
       "  0.8867431374764788,\n",
       "  1.083107689292336,\n",
       "  0.9154436446424309,\n",
       "  0.8654327466391241,\n",
       "  0.8549167710682325,\n",
       "  0.9590157862044746,\n",
       "  0.8016266121257855,\n",
       "  0.9231364883512935,\n",
       "  0.8070053464420315,\n",
       "  0.7910810819386815,\n",
       "  0.8451730668123614,\n",
       "  0.8465989427346641,\n",
       "  0.7674094970095952,\n",
       "  0.7525547074757353,\n",
       "  0.8317528452382029,\n",
       "  0.8280942385504352,\n",
       "  0.793187925943259,\n",
       "  0.8219035819544142,\n",
       "  0.7792159010593489,\n",
       "  0.804285173236149,\n",
       "  0.7074452617346642,\n",
       "  0.767826415429376,\n",
       "  0.740374417476115,\n",
       "  0.8436707541950577,\n",
       "  0.7690808913352188,\n",
       "  0.770382628515335,\n",
       "  0.8205424699476547,\n",
       "  0.8645832958874006,\n",
       "  0.7221817919515509,\n",
       "  0.7534581502825409,\n",
       "  0.6879422347223773,\n",
       "  0.7191299769283449,\n",
       "  0.7877149536025351,\n",
       "  0.7584751909131142,\n",
       "  0.8904871886970829,\n",
       "  0.7239143955703117,\n",
       "  0.7380710065520001,\n",
       "  0.842143934337833,\n",
       "  0.7831504797571962,\n",
       "  0.7101836566501233,\n",
       "  0.7576898447241026,\n",
       "  0.8066610847320996,\n",
       "  0.7474888266795362,\n",
       "  0.6759006738795716,\n",
       "  0.6593683176698539,\n",
       "  0.8110619046821963,\n",
       "  0.7210568637376206,\n",
       "  0.6916222931860814,\n",
       "  0.7597772080510822,\n",
       "  0.769762171627022,\n",
       "  0.7241131184977996,\n",
       "  0.8003016120884755,\n",
       "  0.7432098331926658,\n",
       "  0.7956309342925191,\n",
       "  0.7927933464089305,\n",
       "  0.7774818540062768,\n",
       "  0.6672216987645329,\n",
       "  0.82775372439665,\n",
       "  0.7529023268401024,\n",
       "  0.8018518730183435,\n",
       "  0.8022330195409917,\n",
       "  0.8135055677294066,\n",
       "  0.7984756162814732,\n",
       "  0.7561730779028595,\n",
       "  0.8928649756889797,\n",
       "  0.769017569102865,\n",
       "  0.6979688977875999,\n",
       "  0.7104790607497079,\n",
       "  0.7256921563905693,\n",
       "  0.9451845382260939,\n",
       "  0.8902984224052046,\n",
       "  0.7261142166807288,\n",
       "  0.7140399575588117,\n",
       "  0.7097833771916648,\n",
       "  0.7705542488185035,\n",
       "  0.6724711389690582,\n",
       "  0.8041822408024996,\n",
       "  0.7862727145760332,\n",
       "  0.7587316110924125,\n",
       "  0.7204993863494124,\n",
       "  0.6838025577496578,\n",
       "  0.7288319557384294,\n",
       "  0.761944039730655,\n",
       "  0.9305246669193677,\n",
       "  0.7428394593949564,\n",
       "  0.7307677491086263,\n",
       "  0.6951806531514586,\n",
       "  0.7366407586988263,\n",
       "  0.6376934215986139,\n",
       "  0.7739145619889625,\n",
       "  0.8067104768225498,\n",
       "  0.7878100490339541,\n",
       "  0.8183464374671686,\n",
       "  0.691132952618129,\n",
       "  0.817585015859955,\n",
       "  0.7708916817956274,\n",
       "  0.7983126980347809,\n",
       "  0.6722495895240688,\n",
       "  1.0501488783227417,\n",
       "  0.7624777433773805,\n",
       "  0.7087208677752393,\n",
       "  0.7021857633072898,\n",
       "  0.8815517015464246,\n",
       "  0.922142348044776,\n",
       "  0.7083953405100424,\n",
       "  0.723669544654781,\n",
       "  0.7253218422418369,\n",
       "  0.8044290534179421,\n",
       "  0.9792049175835578,\n",
       "  0.7902941864278956,\n",
       "  0.7338244571655645,\n",
       "  0.7282409062169218,\n",
       "  0.7916951346415133,\n",
       "  0.7245411807473773,\n",
       "  0.8388646203108816,\n",
       "  0.7114910351471991,\n",
       "  0.9179300442864434,\n",
       "  0.7667795174092598,\n",
       "  0.7577511492649743,\n",
       "  0.8029907309641046,\n",
       "  0.7433538433564734,\n",
       "  0.8203603840308723,\n",
       "  1.0948124532807755,\n",
       "  0.7536977187502824,\n",
       "  0.7550069098359105,\n",
       "  0.8148658446726545,\n",
       "  0.7394850688736072,\n",
       "  0.7506370694719756,\n",
       "  0.7613040963572257,\n",
       "  0.9839745231080826,\n",
       "  0.7246404521613371,\n",
       "  0.7086994154673165,\n",
       "  0.8619689290831195,\n",
       "  0.6857840053961596,\n",
       "  0.6850358102790659,\n",
       "  0.8771101925709115,\n",
       "  1.008291839311982,\n",
       "  0.6722301136343486,\n",
       "  0.7253594470760023,\n",
       "  1.027048691181766,\n",
       "  0.7153225114195708,\n",
       "  0.8169332158392626,\n",
       "  1.0047796215652842,\n",
       "  0.7600969951717593,\n",
       "  0.6769829397309709,\n",
       "  0.7016535223350511,\n",
       "  0.7481331930988648,\n",
       "  0.7557227054285977,\n",
       "  0.7707554129873256,\n",
       "  0.7403759155720306,\n",
       "  0.728750168124554,\n",
       "  0.804258145976572,\n",
       "  0.8700055497514576,\n",
       "  0.8429934298650799,\n",
       "  0.8285381315188055,\n",
       "  0.7358036502757982,\n",
       "  0.8434478162433543,\n",
       "  0.8644826000133116,\n",
       "  0.7291028828991528,\n",
       "  1.124898559677419,\n",
       "  0.7701707405643243,\n",
       "  0.7486157521478036],\n",
       " [0.2580885087393083,\n",
       "  0.30922275939010785,\n",
       "  0.33767199702491635,\n",
       "  0.36110078095946446,\n",
       "  0.41446634436593527,\n",
       "  0.3540349572331722,\n",
       "  0.44161398289326886,\n",
       "  0.4788025288211231,\n",
       "  0.48121978430643364,\n",
       "  0.48995909259947934,\n",
       "  0.4867980661956118,\n",
       "  0.5098549646708813,\n",
       "  0.5193380438824842,\n",
       "  0.5371885459278543,\n",
       "  0.5567125325399777,\n",
       "  0.5412792859799181,\n",
       "  0.5570844179992562,\n",
       "  0.5898103384157679,\n",
       "  0.5730754927482336,\n",
       "  0.5831164001487542,\n",
       "  0.5929713648196355,\n",
       "  0.6271848270732614,\n",
       "  0.6236519152101153,\n",
       "  0.6158423205652659,\n",
       "  0.6351803644477501,\n",
       "  0.657121606545184,\n",
       "  0.6597248047601338,\n",
       "  0.6121234659724805,\n",
       "  0.6887318705838602,\n",
       "  0.6746002231312755,\n",
       "  0.6838973596132392,\n",
       "  0.7010040907400521,\n",
       "  0.6970992934176273,\n",
       "  0.6792487913722574,\n",
       "  0.6677203421346225,\n",
       "  0.6751580513201934,\n",
       "  0.6972852361472667,\n",
       "  0.685942729639271,\n",
       "  0.7095574563034586,\n",
       "  0.6561918928969878,\n",
       "  0.7015619189289699,\n",
       "  0.7151357381926366,\n",
       "  0.715321680922276,\n",
       "  0.6907772406098922,\n",
       "  0.7314986984008925,\n",
       "  0.7028635180364448,\n",
       "  0.7443287467460022,\n",
       "  0.7422833767199702,\n",
       "  0.7313127556712532,\n",
       "  0.7326143547787282,\n",
       "  0.7632949051692078,\n",
       "  0.7499070286351803,\n",
       "  0.7463741167720342,\n",
       "  0.7424693194496095,\n",
       "  0.7525102268501301,\n",
       "  0.7458162885831164,\n",
       "  0.7738936407586463,\n",
       "  0.7573447378207512,\n",
       "  0.7831907772406099,\n",
       "  0.7651543324656006,\n",
       "  0.7658981033841576,\n",
       "  0.7439568612867237,\n",
       "  0.7582744514689476,\n",
       "  0.7761249535143175,\n",
       "  0.7534399404983265,\n",
       "  0.7415396058014132,\n",
       "  0.7870955745630346,\n",
       "  0.7731498698400893,\n",
       "  0.8014131647452585,\n",
       "  0.7869096318333954,\n",
       "  0.7685013015991075,\n",
       "  0.7781703235403495,\n",
       "  0.74488657493492,\n",
       "  0.793603570100409,\n",
       "  0.7883971736705094,\n",
       "  0.7556712532539978,\n",
       "  0.7703607288955002,\n",
       "  0.797694310152473,\n",
       "  0.7872815172926738,\n",
       "  0.770918557084418,\n",
       "  0.7898847155076236,\n",
       "  0.80866493120119,\n",
       "  0.8153588694682038,\n",
       "  0.7783562662699889,\n",
       "  0.8110821866865006,\n",
       "  0.8110821866865006,\n",
       "  0.7919300855336556,\n",
       "  0.791558200074377,\n",
       "  0.8056898475269617,\n",
       "  0.7779843808107103,\n",
       "  0.7963927110449981,\n",
       "  0.7906284864261808,\n",
       "  0.7857939754555597,\n",
       "  0.7889550018594274,\n",
       "  0.8214949795462997,\n",
       "  0.7781703235403495,\n",
       "  0.7986240238006694,\n",
       "  0.7850502045370026,\n",
       "  0.7954629973968018,\n",
       "  0.7725920416511715,\n",
       "  0.7902566009669022,\n",
       "  0.7995537374488657,\n",
       "  0.76273707698029,\n",
       "  0.7969505392339159,\n",
       "  0.8201933804388248,\n",
       "  0.8190777240609892,\n",
       "  0.8045741911491261,\n",
       "  0.7671997024916326,\n",
       "  0.761993306061733,\n",
       "  0.8138713276310896,\n",
       "  0.8084789884715508,\n",
       "  0.8161026403867608,\n",
       "  0.7969505392339159,\n",
       "  0.81889178133135,\n",
       "  0.7989959092599479,\n",
       "  0.8066195611751581,\n",
       "  0.8056898475269617,\n",
       "  0.8114540721457791,\n",
       "  0.8322796578653775,\n",
       "  0.8278170323540349,\n",
       "  0.8121978430643362,\n",
       "  0.7724060989215321,\n",
       "  0.8055039047973225,\n",
       "  0.8157307549274824,\n",
       "  0.8343250278914095,\n",
       "  0.8114540721457791,\n",
       "  0.843622164373373,\n",
       "  0.8114540721457791,\n",
       "  0.797694310152473,\n",
       "  0.8019709929341763,\n",
       "  0.7930457419114912,\n",
       "  0.8317218296764597,\n",
       "  0.8034585347712905,\n",
       "  0.8172182967645965,\n",
       "  0.7991818519895872,\n",
       "  0.8287467460022313,\n",
       "  0.7430271476385273,\n",
       "  0.8146150985496468,\n",
       "  0.8378579397545556,\n",
       "  0.83674228337672,\n",
       "  0.7963927110449981,\n",
       "  0.7675715879509111,\n",
       "  0.8205652658981034,\n",
       "  0.8332093715135738,\n",
       "  0.8296764596504277,\n",
       "  0.8172182967645965,\n",
       "  0.7789140944589067,\n",
       "  0.7917441428040164,\n",
       "  0.8242841204908888,\n",
       "  0.821680922275939,\n",
       "  0.8198214949795463,\n",
       "  0.8270732614354779,\n",
       "  0.7986240238006694,\n",
       "  0.8400892525102268,\n",
       "  0.7878393454815916,\n",
       "  0.8244700632205281,\n",
       "  0.8293045741911491,\n",
       "  0.8142432130903682,\n",
       "  0.8311640014875419,\n",
       "  0.8218668650055783,\n",
       "  0.7536258832279658,\n",
       "  0.8114540721457791,\n",
       "  0.8270732614354779,\n",
       "  0.810710301227222,\n",
       "  0.839531424321309,\n",
       "  0.8419486798066196,\n",
       "  0.8322796578653775,\n",
       "  0.7657121606545184,\n",
       "  0.8272592041651171,\n",
       "  0.8458534771290442,\n",
       "  0.795648940126441,\n",
       "  0.8445518780215694,\n",
       "  0.847712904425437,\n",
       "  0.8079211602826329,\n",
       "  0.7447006322052808,\n",
       "  0.8460394198586835,\n",
       "  0.8341390851617702,\n",
       "  0.7500929713648197,\n",
       "  0.8319077724060989,\n",
       "  0.813499442171811,\n",
       "  0.764038676087765,\n",
       "  0.8222387504648568,\n",
       "  0.8523614726664187,\n",
       "  0.8452956489401264,\n",
       "  0.8358125697285236,\n",
       "  0.8278170323540349,\n",
       "  0.8239122350316103,\n",
       "  0.8345109706210487,\n",
       "  0.8497582744514689,\n",
       "  0.817590182223875,\n",
       "  0.7993677947192265,\n",
       "  0.8036444775009297,\n",
       "  0.80866493120119,\n",
       "  0.8365563406470807,\n",
       "  0.801785050204537,\n",
       "  0.8014131647452585,\n",
       "  0.8352547415396058,\n",
       "  0.715321680922276,\n",
       "  0.8172182967645965,\n",
       "  0.8339531424321309])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model,\n",
    "            optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            n_epoch=200,\n",
    "            batch_size=200,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            backup_name=\"lstm_action_classifier.pth.tar\")"
   ]
  },
  {
   "source": [
    "## Load and Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"lstm_action_classifier.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_sequence(model, sequence):\n",
    "    model.eval()\n",
    "    X_batch = sequence.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model.forward(X_batch)\n",
    "        print(y_pred)\n",
    "    return torch.argmax(y_pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 7.6807, -7.4713, -1.3673, -8.8626, -7.3632, -1.4662]],\n       device='cuda:0')\n0\n"
     ]
    }
   ],
   "source": [
    "sequence = train_dataset[45][0].unsqueeze(0)\n",
    "predictions = run_inference_on_sequence(model, sequence)\n",
    "print(predictions.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 6.3528, -7.8725, -2.6291, -6.6467, -6.0407, -5.5711]],\n       device='cuda:0')\n0\n"
     ]
    }
   ],
   "source": [
    "predictions = run_inference_on_sequence(model, sequence)\n",
    "print(predictions.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = ([0.8735486226845841,\n",
    "  0.48130274497343034,\n",
    "  0.3702782436299719,\n",
    "  0.3491351202047991,\n",
    "  0.2769810041340675,\n",
    "  0.20154464583205914,\n",
    "  0.2445332597879415,\n",
    "  0.17224384997431086,\n",
    "  0.1885981674023096,\n",
    "  0.19384880530241444,\n",
    "  0.19337554822639866,\n",
    "  0.16536105377239416,\n",
    "  0.1571138994552154,\n",
    "  0.15273290199140158,\n",
    "  0.10815980746436514,\n",
    "  0.12133629483412643,\n",
    "  0.13833611768088946,\n",
    "  0.1293170387481046,\n",
    "  0.10733797376491747,\n",
    "  0.10590762594786797,\n",
    "  0.0918239034835805,\n",
    "  0.1047938061950121,\n",
    "  0.07058675783115197,\n",
    "  0.08766820735364987,\n",
    "  0.10277271190071632,\n",
    "  0.12322432884653176,\n",
    "  0.08019538524956828,\n",
    "  0.08205282129964776,\n",
    "  0.07426594978620334,\n",
    "  0.0755969595853548,\n",
    "  0.09760807308867492,\n",
    "  0.07207354542732404,\n",
    "  0.0975812500682325,\n",
    "  0.09537964315045605,\n",
    "  0.09366875433450493,\n",
    "  0.04696851966758542,\n",
    "  0.0512240380254569,\n",
    "  0.05459230239451631,\n",
    "  0.08089525441829507,\n",
    "  0.06131623744944018,\n",
    "  0.06199158364649636,\n",
    "  0.051481724839184166,\n",
    "  0.06538778840672245,\n",
    "  0.0629294374865063,\n",
    "  0.04425359480803349,\n",
    "  0.05855927577252546,\n",
    "  0.08699687327160525,\n",
    "  0.07487187519443134,\n",
    "  0.05951084620550851,\n",
    "  0.05211047360593793,\n",
    "  0.0654746382687632,\n",
    "  0.07681369263658207,\n",
    "  0.044812741151172154,\n",
    "  0.04169330514596971,\n",
    "  0.056397179180745924,\n",
    "  0.06540772032921424,\n",
    "  0.07633163423087057,\n",
    "  0.055633779746886776,\n",
    "  0.06838037114775641,\n",
    "  0.053590912112544256,\n",
    "  0.0628714390222539,\n",
    "  0.045393846311621903,\n",
    "  0.05751636609517408,\n",
    "  0.08074402875827821,\n",
    "  0.04321444615804029,\n",
    "  0.05079126861872594,\n",
    "  0.050103869856186006,\n",
    "  0.03507644276016325,\n",
    "  0.03688339269977289,\n",
    "  0.041359042481246576,\n",
    "  0.03544109247172077,\n",
    "  0.03485896200435596,\n",
    "  0.04210835051470699,\n",
    "  0.07559601816384749,\n",
    "  0.04357416177816813,\n",
    "  0.030860471100808837,\n",
    "  0.036541297235294604,\n",
    "  0.03869030817890365,\n",
    "  0.036192047033067894,\n",
    "  0.05795359878571838,\n",
    "  0.040659130261747875,\n",
    "  0.04481880535572273,\n",
    "  0.06331975865306445,\n",
    "  0.04256203787514518,\n",
    "  0.03762714751888046,\n",
    "  0.0230426168100116,\n",
    "  0.03029545205979716,\n",
    "  0.04663788862835142,\n",
    "  0.04618885129772497,\n",
    "  0.06090077068450866,\n",
    "  0.05378697729046914,\n",
    "  0.05114451131438682,\n",
    "  0.0467184322129791,\n",
    "  0.05389563671920387,\n",
    "  0.048165335642040105,\n",
    "  0.04513653215453111,\n",
    "  0.037053695992046956,\n",
    "  0.07355283905870348,\n",
    "  0.05021775581395428,\n",
    "  0.05387003534121017],\n",
    " [0.6489281767955801,\n",
    "  0.8121988950276243,\n",
    "  0.8585193370165746,\n",
    "  0.8681546961325967,\n",
    "  0.891889502762431,\n",
    "  0.9223425414364641,\n",
    "  0.906121546961326,\n",
    "  0.9344530386740332,\n",
    "  0.9319779005524862,\n",
    "  0.927646408839779,\n",
    "  0.925878453038674,\n",
    "  0.9390055248618785,\n",
    "  0.9413480662983426,\n",
    "  0.9398453038674033,\n",
    "  0.9611491712707182,\n",
    "  0.9558895027624309,\n",
    "  0.9517348066298342,\n",
    "  0.9534585635359116,\n",
    "  0.9620773480662983,\n",
    "  0.9622099447513812,\n",
    "  0.9653038674033149,\n",
    "  0.9622541436464088,\n",
    "  0.9747624309392265,\n",
    "  0.9688397790055249,\n",
    "  0.963403314917127,\n",
    "  0.9549613259668508,\n",
    "  0.9732154696132597,\n",
    "  0.9703867403314917,\n",
    "  0.974232044198895,\n",
    "  0.9735690607734807,\n",
    "  0.9656574585635359,\n",
    "  0.9750276243093923,\n",
    "  0.964817679558011,\n",
    "  0.9656132596685083,\n",
    "  0.9657016574585635,\n",
    "  0.9833370165745856,\n",
    "  0.9824530386740331,\n",
    "  0.9800220994475138,\n",
    "  0.9707845303867403,\n",
    "  0.9776795580110498,\n",
    "  0.9775027624309393,\n",
    "  0.9813480662983426,\n",
    "  0.9775911602209945,\n",
    "  0.9772375690607735,\n",
    "  0.9844861878453038,\n",
    "  0.9794475138121547,\n",
    "  0.9695469613259668,\n",
    "  0.9748066298342541,\n",
    "  0.9797569060773481,\n",
    "  0.981524861878453,\n",
    "  0.9777237569060774,\n",
    "  0.9734806629834254,\n",
    "  0.9838232044198895,\n",
    "  0.9848397790055249,\n",
    "  0.980243093922652,\n",
    "  0.9768839779005525,\n",
    "  0.9737016574585635,\n",
    "  0.9805966850828729,\n",
    "  0.9762651933701657,\n",
    "  0.9809060773480663,\n",
    "  0.978342541436464,\n",
    "  0.9840441988950276,\n",
    "  0.97953591160221,\n",
    "  0.9716685082872928,\n",
    "  0.9851491712707182,\n",
    "  0.9826298342541436,\n",
    "  0.9824972375690608,\n",
    "  0.9877127071823204,\n",
    "  0.9868729281767956,\n",
    "  0.9853259668508287,\n",
    "  0.9874475138121547,\n",
    "  0.9878453038674033,\n",
    "  0.9854143646408839,\n",
    "  0.9754696132596685,\n",
    "  0.9849723756906077,\n",
    "  0.9887734806629834,\n",
    "  0.9881546961325967,\n",
    "  0.9861215469613259,\n",
    "  0.9877569060773481,\n",
    "  0.9792707182320441,\n",
    "  0.9848397790055249,\n",
    "  0.984707182320442,\n",
    "  0.9776795580110498,\n",
    "  0.984707182320442,\n",
    "  0.9879779005524861,\n",
    "  0.9927513812154696,\n",
    "  0.9897458563535911,\n",
    "  0.9848839779005525,\n",
    "  0.9849281767955801,\n",
    "  0.9796685082872928,\n",
    "  0.9809502762430939,\n",
    "  0.9824530386740331,\n",
    "  0.9832044198895028,\n",
    "  0.9803314917127072,\n",
    "  0.9834254143646409,\n",
    "  0.9847513812154696,\n",
    "  0.9872265193370166,\n",
    "  0.9749834254143647,\n",
    "  0.9823204419889503,\n",
    "  0.982718232044199],\n",
    " [0.6211841620039762,\n",
    "  0.45876633668316125,\n",
    "  0.5034662859063835,\n",
    "  0.27011767895693073,\n",
    "  0.313655665158899,\n",
    "  0.20805899514211246,\n",
    "  0.3991670820106114,\n",
    "  0.32158903264995403,\n",
    "  0.24071570668090958,\n",
    "  0.2567666179594932,\n",
    "  0.3852716677423312,\n",
    "  0.4597050237921575,\n",
    "  0.17627169369967327,\n",
    "  0.23679502981565434,\n",
    "  0.17220754785561754,\n",
    "  0.18002146034804078,\n",
    "  0.22931813937735357,\n",
    "  0.16827500484832936,\n",
    "  0.15206738518692042,\n",
    "  0.14282209915193997,\n",
    "  0.18685480398813387,\n",
    "  0.11685570925779426,\n",
    "  0.20067864179665781,\n",
    "  0.1411727972821085,\n",
    "  0.1530777178196244,\n",
    "  0.13876111770846125,\n",
    "  0.14196548254774952,\n",
    "  0.2708777552003365,\n",
    "  0.11546770097299551,\n",
    "  0.3046354775860649,\n",
    "  0.10854558102802168,\n",
    "  0.07933161259128273,\n",
    "  0.12404691129011139,\n",
    "  0.38784327351626957,\n",
    "  0.07989218154476951,\n",
    "  0.17622217972792195,\n",
    "  0.09162371938435548,\n",
    "  0.13405118411540942,\n",
    "  0.10392435214123062,\n",
    "  0.10115272597895769,\n",
    "  0.09087322915753134,\n",
    "  0.1590506696334804,\n",
    "  0.22317246896386603,\n",
    "  0.08073770713501655,\n",
    "  0.058100116569523644,\n",
    "  0.14888879645115743,\n",
    "  0.13044291378562928,\n",
    "  0.07736094586014841,\n",
    "  0.11220023536174023,\n",
    "  0.13659915878864518,\n",
    "  0.07129768802977254,\n",
    "  0.20124599370539134,\n",
    "  0.07354985906765003,\n",
    "  0.08835384215030306,\n",
    "  0.21074753585173564,\n",
    "  0.09616019399096185,\n",
    "  0.11539241931760864,\n",
    "  0.120142063001071,\n",
    "  0.0775641988276421,\n",
    "  0.09695740938079764,\n",
    "  0.09058009887954965,\n",
    "  0.06536958760441548,\n",
    "  0.14220434166765117,\n",
    "  0.07930320302022702,\n",
    "  0.08943844097148232,\n",
    "  0.06493218757685769,\n",
    "  0.08265407143692353,\n",
    "  0.0510503365727251,\n",
    "  0.06409464690922057,\n",
    "  0.0902351198168243,\n",
    "  0.06698930089201968,\n",
    "  0.07941066838737426,\n",
    "  0.27309605946031246,\n",
    "  0.10141229745728485,\n",
    "  0.08438794725363463,\n",
    "  0.06564738088682306,\n",
    "  0.07934415308124455,\n",
    "  0.11631594676814944,\n",
    "  0.11903673065906457,\n",
    "  0.09058221255546416,\n",
    "  0.12191057274405673,\n",
    "  0.1233115003656721,\n",
    "  0.10492566676817347,\n",
    "  0.11040246008239304,\n",
    "  0.0784957536592911,\n",
    "  0.05796452499581156,\n",
    "  0.06605394684588234,\n",
    "  0.08701971364890956,\n",
    "  0.23499713032428876,\n",
    "  0.14679802382028134,\n",
    "  0.0931847726414798,\n",
    "  0.1281534519503502,\n",
    "  0.09721326736259316,\n",
    "  0.0960857172550666,\n",
    "  0.1704642465234064,\n",
    "  0.11685481813445657,\n",
    "  0.14966437045596195,\n",
    "  0.22490323309782892,\n",
    "  0.09092609374476246,\n",
    "  0.08504296933522287],\n",
    " [0.7440445139975657,\n",
    "  0.8125543383759346,\n",
    "  0.8101199791340636,\n",
    "  0.9012345679012346,\n",
    "  0.869414014953921,\n",
    "  0.9231438010780734,\n",
    "  0.8497652582159625,\n",
    "  0.892018779342723,\n",
    "  0.9128847157016171,\n",
    "  0.9021039819161885,\n",
    "  0.8753260302556077,\n",
    "  0.8575899843505478,\n",
    "  0.9285341679707877,\n",
    "  0.9189706138062945,\n",
    "  0.9384454877412624,\n",
    "  0.9398365501651886,\n",
    "  0.9163623717614328,\n",
    "  0.94679186228482,\n",
    "  0.9419231438010781,\n",
    "  0.942792557816032,\n",
    "  0.9297513475917232,\n",
    "  0.952529994783516,\n",
    "  0.9400104329681794,\n",
    "  0.9497478699356634,\n",
    "  0.9521822291775344,\n",
    "  0.9539210572074421,\n",
    "  0.9410537297861241,\n",
    "  0.9031472787341331,\n",
    "  0.9601808381151105,\n",
    "  0.8941053729786124,\n",
    "  0.9612241349330551,\n",
    "  0.9688749782646496,\n",
    "  0.9575725960702487,\n",
    "  0.8626325856372805,\n",
    "  0.9680055642496957,\n",
    "  0.9514866979655712,\n",
    "  0.9640062597809077,\n",
    "  0.952529994783516,\n",
    "  0.9575725960702487,\n",
    "  0.9641801425838985,\n",
    "  0.9650495565988524,\n",
    "  0.9384454877412624,\n",
    "  0.9297513475917232,\n",
    "  0.9683533298556772,\n",
    "  0.9746131107633456,\n",
    "  0.9436619718309859,\n",
    "  0.9544427056164145,\n",
    "  0.9688749782646496,\n",
    "  0.9613980177360459,\n",
    "  0.9547904712223961,\n",
    "  0.9721787515214745,\n",
    "  0.9337506520605112,\n",
    "  0.9754825247782994,\n",
    "  0.968527212658668,\n",
    "  0.9198400278212485,\n",
    "  0.9629629629629629,\n",
    "  0.9573987132672579,\n",
    "  0.9657450878108155,\n",
    "  0.97339593114241,\n",
    "  0.96452790818988,\n",
    "  0.9657450878108155,\n",
    "  0.9735698139454008,\n",
    "  0.9497478699356634,\n",
    "  0.9697443922796035,\n",
    "  0.9680055642496957,\n",
    "  0.9721787515214745,\n",
    "  0.970961571900539,\n",
    "  0.9780907668231612,\n",
    "  0.974960876369327,\n",
    "  0.9699182750825943,\n",
    "  0.9723526343244653,\n",
    "  0.9711354547035298,\n",
    "  0.9167101373674144,\n",
    "  0.9673100330377326,\n",
    "  0.9711354547035298,\n",
    "  0.9798295948530691,\n",
    "  0.9725265171274561,\n",
    "  0.9643540253868892,\n",
    "  0.9551382368283776,\n",
    "  0.9697443922796035,\n",
    "  0.9587897756911842,\n",
    "  0.9591375412971657,\n",
    "  0.970961571900539,\n",
    "  0.9648756737958616,\n",
    "  0.9720048687184838,\n",
    "  0.9810467744740046,\n",
    "  0.975830290384281,\n",
    "  0.9728742827334377,\n",
    "  0.9300991131977048,\n",
    "  0.9553121196313684,\n",
    "  0.9706138062945575,\n",
    "  0.9601808381151105,\n",
    "  0.9674839158407234,\n",
    "  0.9673100330377326,\n",
    "  0.9394887845592071,\n",
    "  0.9636584941749261,\n",
    "  0.9558337680403408,\n",
    "  0.9257520431229351,\n",
    "  0.9659189706138063,\n",
    "  0.970961571900539])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}