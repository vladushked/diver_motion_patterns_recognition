{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \" + DEVICE)\n",
    "if torch.backends.cudnn.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_with_norm(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, header=None))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, header=None))\n",
    "    \n",
    "    # Shift labels in PyTorch classification models labels start from 0\n",
    "    y_data = y_data - 1\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, sep=\"\\t\", header=None, dtype=np.float32))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, sep=\"\\t\", header=None, dtype=np.int_))\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 16\n",
    "\n",
    "X_all, y_all = read_data(\"data/shortened_shuffled_poses.csv\",\"data/shortened_shuffled_labels.csv\", SEQ_LENGTH)\n",
    "val_size = int(y_all.shape[0] * 0.2)\n",
    "X_val, y_val = X_all[-val_size:], y_all[-val_size:]\n",
    "X_train, y_train = X_all[:-val_size], y_all[:-val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                               torch.tensor(y_train, dtype=torch.long).squeeze())\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                             torch.tensor(y_val, dtype=torch.long).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_classes, lstm_hidden_dim=256, fc_hidden_dim=256, n_lstm_layers=2):\n",
    "        super(RnnClassifier, self).__init__()\n",
    "        \n",
    "        self._lstm = nn.RNN(input_size=input_dim,\n",
    "                             hidden_size=lstm_hidden_dim,\n",
    "                             num_layers=n_lstm_layers,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        self._fc = nn.Sequential(nn.Linear(lstm_hidden_dim, fc_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc_hidden_dim, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self._lstm.forward(x)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        fc_output = self._fc.forward(lstm_output)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_classes, lstm_hidden_dim=256, fc_hidden_dim=256, n_lstm_layers=2):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        \n",
    "        self._lstm = nn.LSTM(input_size=input_dim,\n",
    "                             hidden_size=lstm_hidden_dim,\n",
    "                             num_layers=n_lstm_layers,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        self._fc = nn.Sequential(nn.Linear(lstm_hidden_dim, fc_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc_hidden_dim, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self._lstm.forward(x)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        fc_output = self._fc.forward(lstm_output)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GruClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_classes, lstm_hidden_dim=256, fc_hidden_dim=256, n_lstm_layers=2):\n",
    "        super(GruClassifier, self).__init__()\n",
    "        \n",
    "        self._lstm = nn.GRU(input_size=input_dim,\n",
    "                             hidden_size=lstm_hidden_dim,\n",
    "                             num_layers=n_lstm_layers,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        self._fc = nn.Sequential(nn.Linear(lstm_hidden_dim, fc_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc_hidden_dim, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self._lstm.forward(x)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        fc_output = self._fc.forward(lstm_output)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, criterion, batches, phase='train'):\n",
    "    is_train = phase == 'train'\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_predictions = 0\n",
    "    \n",
    "    correct_predictions = 0\n",
    "\n",
    "    for X_batch, y_batch in batches:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = criterion.forward(y_pred, y_batch)\n",
    "    \n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * y_batch.shape[0]\n",
    "        correct_predictions += (torch.argmax(y_pred, dim=1) == y_batch).sum().item()\n",
    "        n_predictions += y_batch.shape[0]\n",
    "\n",
    "    epoch_loss = epoch_loss / n_predictions\n",
    "    epoch_accuracy = correct_predictions / n_predictions\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, n_epoch, batch_size, train_dataset, val_dataset, backup_name):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_accuracy = np.inf\n",
    "    \n",
    "    train_batches = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    val_batches = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_accuracy = run_epoch(model, optimizer, criterion, train_batches, phase='train')\n",
    "        val_loss, val_accuracy = run_epoch(model, optimizer, criterion, val_batches, phase='val')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), backup_name)\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        print(\"Train loss: \" + str(train_loss) + \", accuracy: \" + str(train_accuracy))\n",
    "        print(\"Val loss: \" + str(val_loss) + \", accuracy: \" + str(val_accuracy) + \"\\n\\n\")\n",
    "        \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies, best_val_loss, best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "source": [
    "N_CLASSES = 8\n",
    "N_EPOCHS = 500\n",
    "INPUT_DIM = X_train.shape[2]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 27,
   "outputs": []
  },
  {
   "source": [
    "## RNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\nTrain loss: 1.8566540167120786, accuracy: 0.2761979841207275\nVal loss: 1.7618220125562734, accuracy: 0.28857850867312457\n\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training/rnn_training_vals'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0f3d6f50da4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train_losses\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_accuracies\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_losses\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_accuracies\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_accuracies\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training/rnn_training_vals\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3384\u001b[0m         )\n\u001b[1;32m   3385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3386\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3387\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3388\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training/rnn_training_vals'"
     ]
    }
   ],
   "source": [
    "model = RnnClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, train_accuracies, val_losses, val_accuracies, best_val_loss, best_val_accuracy = train_model(model,\n",
    "            optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            n_epoch=N_EPOCHS,\n",
    "            batch_size=500,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            backup_name=\"rnn_action_classifier.pth.tar\")\n",
    "\n",
    "training = pd.DataFrame({\"train_losses\":train_losses, \"train_accuracies\":train_accuracies, \"val_losses\":val_losses, \"val_accuracies\":val_accuracies})\n",
    "training.to_csv(\"rnn_training_vals.csv\",  sep=\"\\t\", index=False)\n",
    "print(best_val_loss)\n",
    "print(best_val_accuracy)"
   ]
  },
  {
   "source": [
    "## LSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\nTrain loss: 1.7322357745111254, accuracy: 0.3298046061152092\nVal loss: 1.3900967147281025, accuracy: 0.4548321694075242\n\n\n"
     ]
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, train_accuracies, val_losses, val_accuracies, best_val_loss, best_val_accuracy = train_model(model,\n",
    "            optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            n_epoch=N_EPOCHS,\n",
    "            batch_size=500,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            backup_name=\"lstm_action_classifier.pth.tar\")\n",
    "\n",
    "training = pd.DataFrame({\"train_losses\":train_losses, \"train_accuracies\":train_accuracies, \"val_losses\":val_losses, \"val_accuracies\":val_accuracies})\n",
    "training.to_csv(\"lstm_training_vals.csv\",  sep=\"\\t\", index=False)\n",
    "print(best_val_loss)\n",
    "print(best_val_accuracy)"
   ]
  },
  {
   "source": [
    "## GRU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\nTrain loss: 1.727622659277947, accuracy: 0.3471479249957768\nVal loss: 1.5221325417954956, accuracy: 0.45122775399864834\n\n\n"
     ]
    }
   ],
   "source": [
    "model = GruClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, train_accuracies, val_losses, val_accuracies, best_val_loss, best_val_accuracy = train_model(model,\n",
    "            optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            n_epoch=N_EPOCHS,\n",
    "            batch_size=500,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            backup_name=\"lstm_action_classifier.pth.tar\")\n",
    "\n",
    "training = pd.DataFrame({\"train_losses\":train_losses, \"train_accuracies\":train_accuracies, \"val_losses\":val_losses, \"val_accuracies\":val_accuracies})\n",
    "training.to_csv(\"gru_training_vals.csv\",  sep=\"\\t\", index=False)\n",
    "print(best_val_loss)\n",
    "print(best_val_accuracy)"
   ]
  },
  {
   "source": [
    "## Load and Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"lstm_action_classifier.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_sequence(model, sequence):\n",
    "    model.eval()\n",
    "    X_batch = sequence.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model.forward(X_batch)\n",
    "        print(y_pred)\n",
    "    return torch.argmax(y_pred, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38564bit29e064361c954e90adf267cf1683fa25",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}