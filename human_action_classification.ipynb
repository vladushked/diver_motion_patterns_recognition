{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \" + DEVICE)\n",
    "if torch.backends.cudnn.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_with_norm(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, header=None))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, header=None))\n",
    "    \n",
    "    # Shift labels in PyTorch classification models labels start from 0\n",
    "    y_data = y_data - 1\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, sep=\"\\t\", header=None, dtype=np.float32))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, sep=\"\\t\", header=None, dtype=np.int_))\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 16\n",
    "\n",
    "X_all, y_all = read_data(\"data/shortened_shuffled_poses.csv\",\"data/shortened_shuffled_labels.csv\", SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4439"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "val_size = int(y_all.shape[0] * 0.2)\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = X_all[-val_size:], y_all[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_all[:-val_size], y_all[:-val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4439, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                               torch.tensor(y_train, dtype=torch.long).squeeze())\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                             torch.tensor(y_val, dtype=torch.long).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_classes, lstm_hidden_dim=256, fc_hidden_dim=256, n_lstm_layers=2):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        \n",
    "        self._lstm = nn.LSTM(input_size=input_dim,\n",
    "                             hidden_size=lstm_hidden_dim,\n",
    "                             num_layers=n_lstm_layers,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        self._fc = nn.Sequential(nn.Linear(lstm_hidden_dim, fc_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc_hidden_dim, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self._lstm.forward(x)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        fc_output = self._fc.forward(lstm_output)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, criterion, batches, phase='train'):\n",
    "    is_train = phase == 'train'\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_predictions = 0\n",
    "    \n",
    "    correct_predictions = 0\n",
    "\n",
    "    for X_batch, y_batch in batches:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = criterion.forward(y_pred, y_batch)\n",
    "    \n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * y_batch.shape[0]\n",
    "        correct_predictions += (torch.argmax(y_pred, dim=1) == y_batch).sum().item()\n",
    "        n_predictions += y_batch.shape[0]\n",
    "\n",
    "    epoch_loss = epoch_loss / n_predictions\n",
    "    epoch_accuracy = correct_predictions / n_predictions\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, n_epoch, batch_size, train_dataset, val_dataset, backup_name):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    train_batches = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    val_batches = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_accuracy = run_epoch(model, optimizer, criterion, train_batches, phase='train')\n",
    "        val_loss, val_accuracy = run_epoch(model, optimizer, criterion, val_batches, phase='val')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), backup_name)\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        print(\"Train loss: \" + str(train_loss) + \", accuracy: \" + str(train_accuracy))\n",
    "        print(\"Val loss: \" + str(val_loss) + \", accuracy: \" + str(val_accuracy) + \"\\n\\n\")\n",
    "        \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "source": [
    "N_CLASSES = 10\n",
    "INPUT_DIM = X_train.shape[2]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "Train loss: 1.80303891634537, accuracy: 0.366405766090433\n",
      "Val loss: 1.4474935686032389, accuracy: 0.47668393782383417\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 1.4037343258457387, accuracy: 0.4960864913564953\n",
      "Val loss: 1.2433718643255076, accuracy: 0.5496733498535706\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 1.2387259473228207, accuracy: 0.5629821498958275\n",
      "Val loss: 1.2641021079591708, accuracy: 0.554629421040775\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 1.113992998222623, accuracy: 0.6096063967565741\n",
      "Val loss: 1.1171691065625207, accuracy: 0.5996846136517233\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 1.0959734790322078, accuracy: 0.6155188918294949\n",
      "Val loss: 1.005354053803891, accuracy: 0.6451903581887812\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 0.9884114109742842, accuracy: 0.6567374289092854\n",
      "Val loss: 0.9997192203501748, accuracy: 0.6456409101148908\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 0.9441809455005494, accuracy: 0.6678867053325075\n",
      "Val loss: 0.8854842624381792, accuracy: 0.6913719306150033\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 0.901986074075753, accuracy: 0.6846106199673405\n",
      "Val loss: 0.8977654095060199, accuracy: 0.6828114440189231\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 0.8663819905833434, accuracy: 0.6954783490061377\n",
      "Val loss: 0.9580965711877646, accuracy: 0.665465194863708\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 0.8548426974852804, accuracy: 0.6992510839574301\n",
      "Val loss: 0.8260981624366518, accuracy: 0.7084929038071638\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 0.8068731064845852, accuracy: 0.7189594008671659\n",
      "Val loss: 0.7989573605185721, accuracy: 0.7217841856273935\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Train loss: 0.7634081874011074, accuracy: 0.7379356945773974\n",
      "Val loss: 0.7557464672869781, accuracy: 0.7377787790042802\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Train loss: 0.7432278734592243, accuracy: 0.7409200968523002\n",
      "Val loss: 0.7726642539920665, accuracy: 0.728317188555981\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Train loss: 0.732029881135147, accuracy: 0.7454248550030971\n",
      "Val loss: 0.7223412444205347, accuracy: 0.7463392656003605\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Train loss: 0.701115178937035, accuracy: 0.757981868348443\n",
      "Val loss: 0.7615802416825085, accuracy: 0.7312457760756927\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Train loss: 0.6872674304257979, accuracy: 0.7647953150515232\n",
      "Val loss: 0.7487619889832531, accuracy: 0.7400315386348276\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Train loss: 0.6646099252652207, accuracy: 0.7706515006475589\n",
      "Val loss: 0.6822154661050974, accuracy: 0.7751745888713675\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Train loss: 0.6337761553875451, accuracy: 0.7840531561461794\n",
      "Val loss: 0.6683433025397275, accuracy: 0.7675152061275062\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Train loss: 0.6317625433765384, accuracy: 0.7781969705501436\n",
      "Val loss: 0.6428313181458843, accuracy: 0.7805812119846812\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Train loss: 0.6148760630120645, accuracy: 0.786080297314038\n",
      "Val loss: 0.6279116535165283, accuracy: 0.7866636629871593\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Train loss: 0.5836928521998126, accuracy: 0.7954839799538262\n",
      "Val loss: 0.6980185611985454, accuracy: 0.7560261320117143\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "Train loss: 0.5746500295886277, accuracy: 0.8050565910242694\n",
      "Val loss: 0.6550223375795445, accuracy: 0.772020725388601\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "Train loss: 0.5741300609400563, accuracy: 0.8010023086885523\n",
      "Val loss: 0.6273116889663795, accuracy: 0.7859878350979951\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "Train loss: 0.5412345720784703, accuracy: 0.8137282504645532\n",
      "Val loss: 0.6061158042067906, accuracy: 0.792070286100473\n",
      "\n",
      "\n",
      "Epoch: 24\n",
      "Train loss: 0.5436179608608381, accuracy: 0.8109690860971902\n",
      "Val loss: 0.5990448951936032, accuracy: 0.7943230457310205\n",
      "\n",
      "\n",
      "Epoch: 25\n",
      "Train loss: 0.5384718254481264, accuracy: 0.809730277605721\n",
      "Val loss: 0.5955919695346306, accuracy: 0.793872493804911\n",
      "\n",
      "\n",
      "Epoch: 26\n",
      "Train loss: 0.5053915903068715, accuracy: 0.8249338363646602\n",
      "Val loss: 0.5874055830925857, accuracy: 0.803559360216265\n",
      "\n",
      "\n",
      "Epoch: 27\n",
      "Train loss: 0.5171912114955437, accuracy: 0.818176699138465\n",
      "Val loss: 0.6068623953237273, accuracy: 0.7949988736201847\n",
      "\n",
      "\n",
      "Epoch: 28\n",
      "Train loss: 0.508954534814449, accuracy: 0.8229066951968016\n",
      "Val loss: 0.5813132884481464, accuracy: 0.8089659833295787\n",
      "\n",
      "\n",
      "Epoch: 29\n",
      "Train loss: 0.4599814714528371, accuracy: 0.8359141843572273\n",
      "Val loss: 0.5905775367663126, accuracy: 0.8033340842532102\n",
      "\n",
      "\n",
      "Epoch: 30\n",
      "Train loss: 0.4757751004455849, accuracy: 0.833492876851174\n",
      "Val loss: 0.589427463856332, accuracy: 0.8013066005857175\n",
      "\n",
      "\n",
      "Epoch: 31\n",
      "Train loss: 0.4448489431671291, accuracy: 0.8443606058899713\n",
      "Val loss: 0.580138545230746, accuracy: 0.8127956747015094\n",
      "\n",
      "\n",
      "Epoch: 32\n",
      "Train loss: 0.41668850264373924, accuracy: 0.8551720254518835\n",
      "Val loss: 0.5291919682739715, accuracy: 0.8274386123000675\n",
      "\n",
      "\n",
      "Epoch: 33\n",
      "Train loss: 0.46209545161240917, accuracy: 0.8375471591868912\n",
      "Val loss: 0.5455858888162044, accuracy: 0.8269880603739581\n",
      "\n",
      "\n",
      "Epoch: 34\n",
      "Train loss: 0.4417306220060238, accuracy: 0.8435159637366969\n",
      "Val loss: 0.5584284485420361, accuracy: 0.8033340842532102\n",
      "\n",
      "\n",
      "Epoch: 35\n",
      "Train loss: 0.4178275327819701, accuracy: 0.8521876231769807\n",
      "Val loss: 0.5426142578136817, accuracy: 0.8170759179995495\n",
      "\n",
      "\n",
      "Epoch: 36\n",
      "Train loss: 0.4127360366625093, accuracy: 0.8537079790528745\n",
      "Val loss: 0.5963983912944901, accuracy: 0.7958999774724037\n",
      "\n",
      "\n",
      "Epoch: 37\n",
      "Train loss: 0.37493028578454024, accuracy: 0.8664339208288755\n",
      "Val loss: 0.5445776810568714, accuracy: 0.8114440189231809\n",
      "\n",
      "\n",
      "Epoch: 38\n",
      "Train loss: 0.37042964168836473, accuracy: 0.8678979672278845\n",
      "Val loss: 0.5350776797722363, accuracy: 0.8292408200045055\n",
      "\n",
      "\n",
      "Epoch: 39\n",
      "Train loss: 0.41288805471272666, accuracy: 0.8532575032377949\n",
      "Val loss: 0.4966720571574877, accuracy: 0.8373507546744763\n",
      "\n",
      "\n",
      "Epoch: 40\n",
      "Train loss: 0.3457054301716265, accuracy: 0.8773016498676728\n",
      "Val loss: 0.47695946795469363, accuracy: 0.8416309979725163\n",
      "\n",
      "\n",
      "Epoch: 41\n",
      "Train loss: 0.3593086609936249, accuracy: 0.8724027253786812\n",
      "Val loss: 0.5138190994662297, accuracy: 0.8283397161522865\n",
      "\n",
      "\n",
      "Epoch: 42\n",
      "Train loss: 0.3450419474285098, accuracy: 0.8764570077143984\n",
      "Val loss: 0.49025888178740085, accuracy: 0.8389276864158595\n",
      "\n",
      "\n",
      "Epoch: 43\n",
      "Train loss: 0.3509844822455744, accuracy: 0.8748240328847345\n",
      "Val loss: 0.5395297097794562, accuracy: 0.8204550574453706\n",
      "\n",
      "\n",
      "Epoch: 44\n",
      "Train loss: 0.3608254215505063, accuracy: 0.8709949884565572\n",
      "Val loss: 0.5474331980085663, accuracy: 0.8161748141473305\n",
      "\n",
      "\n",
      "Epoch: 45\n",
      "Train loss: 0.32321866195047083, accuracy: 0.885410214539107\n",
      "Val loss: 0.4946967218609161, accuracy: 0.8416309979725163\n",
      "\n",
      "\n",
      "Epoch: 46\n",
      "Train loss: 0.3203339093698361, accuracy: 0.8851849766315671\n",
      "Val loss: 0.4783106492859578, accuracy: 0.8492903807163775\n",
      "\n",
      "\n",
      "Epoch: 47\n",
      "Train loss: 0.2833788674776803, accuracy: 0.896503181485444\n",
      "Val loss: 0.45263380365453343, accuracy: 0.8528947961252534\n",
      "\n",
      "\n",
      "Epoch: 48\n",
      "Train loss: 0.30587281774471686, accuracy: 0.8896897347823639\n",
      "Val loss: 0.5230448802706088, accuracy: 0.8256364045956296\n",
      "\n",
      "\n",
      "Epoch: 49\n",
      "Train loss: 0.3502583837205745, accuracy: 0.8756686750380089\n",
      "Val loss: 0.658141210530894, accuracy: 0.7979274611398963\n",
      "\n",
      "\n",
      "Epoch: 50\n",
      "Train loss: 0.31298886600247683, accuracy: 0.8892392589672842\n",
      "Val loss: 0.4236070333259765, accuracy: 0.8675377337238117\n",
      "\n",
      "\n",
      "Epoch: 51\n",
      "Train loss: 0.2569693112020604, accuracy: 0.9064136494171969\n",
      "Val loss: 0.49188948624250617, accuracy: 0.8414057220094616\n",
      "\n",
      "\n",
      "Epoch: 52\n",
      "Train loss: 0.3003723449114375, accuracy: 0.8918294949039923\n",
      "Val loss: 0.46741968354386076, accuracy: 0.8479387249380491\n",
      "\n",
      "\n",
      "Epoch: 53\n",
      "Train loss: 0.2584383263533714, accuracy: 0.9052874598794978\n",
      "Val loss: 0.45616021175646626, accuracy: 0.8569497634602388\n",
      "\n",
      "\n",
      "Epoch: 54\n",
      "Train loss: 0.25902344615443734, accuracy: 0.9062447209865421\n",
      "Val loss: 0.47207914540885293, accuracy: 0.8477134489749943\n",
      "\n",
      "\n",
      "Epoch: 55\n",
      "Train loss: 0.28899548442099665, accuracy: 0.8939129455487359\n",
      "Val loss: 0.48379578678208873, accuracy: 0.8400540662311331\n",
      "\n",
      "\n",
      "Epoch: 56\n",
      "Train loss: 0.274865978650463, accuracy: 0.8987555605608424\n",
      "Val loss: 0.4435110294381142, accuracy: 0.8616805586843884\n",
      "\n",
      "\n",
      "Epoch: 57\n",
      "Train loss: 0.25393234238719087, accuracy: 0.9066388873247367\n",
      "Val loss: 0.48227026046839727, accuracy: 0.8450101374183374\n",
      "\n",
      "\n",
      "Epoch: 58\n",
      "Train loss: 0.26315548078079026, accuracy: 0.9019652007432851\n",
      "Val loss: 0.5207819842921637, accuracy: 0.8330705113764362\n",
      "\n",
      "\n",
      "Epoch: 59\n",
      "Train loss: 0.27251241422966865, accuracy: 0.9019652007432851\n",
      "Val loss: 0.4971741435629313, accuracy: 0.84275737778779\n",
      "\n",
      "\n",
      "Epoch: 60\n",
      "Train loss: 0.2761606596478659, accuracy: 0.8989244889914972\n",
      "Val loss: 0.5106921405587064, accuracy: 0.8323946834872719\n",
      "\n",
      "\n",
      "Epoch: 61\n",
      "Train loss: 0.2845863891826623, accuracy: 0.8947575877020102\n",
      "Val loss: 0.46906156129304233, accuracy: 0.856724487497184\n",
      "\n",
      "\n",
      "Epoch: 62\n",
      "Train loss: 0.23196915816069924, accuracy: 0.9178444732248437\n",
      "Val loss: 0.5091785628077521, accuracy: 0.8382518585266951\n",
      "\n",
      "\n",
      "Epoch: 63\n",
      "Train loss: 0.20248626143090984, accuracy: 0.9257277999887381\n",
      "Val loss: 0.4298581402294806, accuracy: 0.8682135616129759\n",
      "\n",
      "\n",
      "Epoch: 64\n",
      "Train loss: 0.1969061748818628, accuracy: 0.9289374401711809\n",
      "Val loss: 0.4670009634923924, accuracy: 0.858526695201622\n",
      "\n",
      "\n",
      "Epoch: 65\n",
      "Train loss: 0.26302567790425757, accuracy: 0.9041612703417985\n",
      "Val loss: 0.5215540677745429, accuracy: 0.8438837576030638\n",
      "\n",
      "\n",
      "Epoch: 66\n",
      "Train loss: 0.25745580640956955, accuracy: 0.9046117461568782\n",
      "Val loss: 0.49143780906556506, accuracy: 0.8429826537508448\n",
      "\n",
      "\n",
      "Epoch: 67\n",
      "Train loss: 0.23126262393568447, accuracy: 0.9159299510107551\n",
      "Val loss: 0.4731037810757021, accuracy: 0.8612300067582789\n",
      "\n",
      "\n",
      "Epoch: 68\n",
      "Train loss: 0.20090494223546926, accuracy: 0.9270229179570921\n",
      "Val loss: 0.4148942765337735, accuracy: 0.8697904933543591\n",
      "\n",
      "\n",
      "Epoch: 69\n",
      "Train loss: 0.1937645358636617, accuracy: 0.9292752970324906\n",
      "Val loss: 0.4609526236816203, accuracy: 0.8542464519035818\n",
      "\n",
      "\n",
      "Epoch: 70\n",
      "Train loss: 0.18456029613729977, accuracy: 0.9347373162903316\n",
      "Val loss: 0.4743014323896695, accuracy: 0.8569497634602388\n",
      "\n",
      "\n",
      "Epoch: 71\n",
      "Train loss: 0.23032239805947916, accuracy: 0.9140154287966665\n",
      "Val loss: 0.5047631316100136, accuracy: 0.8472628970488849\n",
      "\n",
      "\n",
      "Epoch: 72\n",
      "Train loss: 0.2339807598952258, accuracy: 0.9143532856579762\n",
      "Val loss: 0.4955551847194492, accuracy: 0.8551475557558008\n",
      "\n",
      "\n",
      "Epoch: 73\n",
      "Train loss: 0.18861370293519084, accuracy: 0.9305704150008447\n",
      "Val loss: 0.43829638110970986, accuracy: 0.8720432529849065\n",
      "\n",
      "\n",
      "Epoch: 74\n",
      "Train loss: 0.17996031516971295, accuracy: 0.9345120783827918\n",
      "Val loss: 0.4612131391889637, accuracy: 0.8614552827213336\n",
      "\n",
      "\n",
      "Epoch: 75\n",
      "Train loss: 0.23709531102311243, accuracy: 0.9158736415338702\n",
      "Val loss: 0.46043153562694017, accuracy: 0.8580761432755125\n",
      "\n",
      "\n",
      "Epoch: 76\n",
      "Train loss: 0.1787402459653999, accuracy: 0.9354130300129512\n",
      "Val loss: 0.4657808991535528, accuracy: 0.858526695201622\n",
      "\n",
      "\n",
      "Epoch: 77\n",
      "Train loss: 0.20312179704578792, accuracy: 0.9253899431274284\n",
      "Val loss: 0.4791493122307506, accuracy: 0.8623563865735526\n",
      "\n",
      "\n",
      "Epoch: 78\n",
      "Train loss: 0.15840054463041991, accuracy: 0.9408187397939073\n",
      "Val loss: 0.4457519395336908, accuracy: 0.8740707366523992\n",
      "\n",
      "\n",
      "Epoch: 79\n",
      "Train loss: 0.20632318518097198, accuracy: 0.9248268483585788\n",
      "Val loss: 0.4613600557627617, accuracy: 0.8589772471277315\n",
      "\n",
      "\n",
      "Epoch: 80\n",
      "Train loss: 0.21898700621937087, accuracy: 0.9215045892223661\n",
      "Val loss: 0.49872109132136383, accuracy: 0.8542464519035818\n",
      "\n",
      "\n",
      "Epoch: 81\n",
      "Train loss: 0.214652959892755, accuracy: 0.9222929218987556\n",
      "Val loss: 0.43672622971557284, accuracy: 0.8760982203198918\n",
      "\n",
      "\n",
      "Epoch: 82\n",
      "Train loss: 0.2139570346089847, accuracy: 0.9215608986992511\n",
      "Val loss: 0.4553587650038901, accuracy: 0.8673124577607569\n",
      "\n",
      "\n",
      "Epoch: 83\n",
      "Train loss: 0.16850524549381146, accuracy: 0.9387915986260488\n",
      "Val loss: 0.41284933539225355, accuracy: 0.8896147781031764\n",
      "\n",
      "\n",
      "Epoch: 84\n",
      "Train loss: 0.15904814216848817, accuracy: 0.941269215608987\n",
      "Val loss: 0.46270157354161073, accuracy: 0.8623563865735526\n",
      "\n",
      "\n",
      "Epoch: 85\n",
      "Train loss: 0.1629085557964164, accuracy: 0.9384537417647391\n",
      "Val loss: 0.4082260644631924, accuracy: 0.8916422617706691\n",
      "\n",
      "\n",
      "Epoch: 86\n",
      "Train loss: 0.16743021237416555, accuracy: 0.9406498113632524\n",
      "Val loss: 0.4738571802408047, accuracy: 0.8641585942779906\n",
      "\n",
      "\n",
      "Epoch: 87\n",
      "Train loss: 0.16878019023236274, accuracy: 0.9369333858888451\n",
      "Val loss: 0.4581707668991931, accuracy: 0.8702410452804685\n",
      "\n",
      "\n",
      "Epoch: 88\n",
      "Train loss: 0.1512706527550738, accuracy: 0.9436342136381553\n",
      "Val loss: 0.44730614353660314, accuracy: 0.8772246001351656\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7023da60239a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_losses, train_accuracies, val_losses, val_accuracies= train_model(model,\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-dbb17172c701>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, n_epoch, batch_size, train_dataset, val_dataset, backup_name)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-dbb17172c701>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, optimizer, criterion, batches, phase)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mn_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, train_accuracies, val_losses, val_accuracies= train_model(model,\n",
    "            optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            n_epoch=200,\n",
    "            batch_size=200,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            backup_name=\"lstm_action_classifier.pth.tar\")"
   ]
  },
  {
   "source": [
    "## Load and Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"lstm_action_classifier.pth.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_sequence(model, sequence):\n",
    "    model.eval()\n",
    "    X_batch = sequence.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model.forward(X_batch)\n",
    "        print(y_pred)\n",
    "    return torch.argmax(y_pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 7.6807, -7.4713, -1.3673, -8.8626, -7.3632, -1.4662]],\n       device='cuda:0')\n0\n"
     ]
    }
   ],
   "source": [
    "sequence = train_dataset[45][0].unsqueeze(0)\n",
    "predictions = run_inference_on_sequence(model, sequence)\n",
    "print(predictions.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 6.3528, -7.8725, -2.6291, -6.6467, -6.0407, -5.5711]],\n       device='cuda:0')\n0\n"
     ]
    }
   ],
   "source": [
    "predictions = run_inference_on_sequence(model, sequence)\n",
    "print(predictions.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = ([0.8735486226845841,\n",
    "  0.48130274497343034,\n",
    "  0.3702782436299719,\n",
    "  0.3491351202047991,\n",
    "  0.2769810041340675,\n",
    "  0.20154464583205914,\n",
    "  0.2445332597879415,\n",
    "  0.17224384997431086,\n",
    "  0.1885981674023096,\n",
    "  0.19384880530241444,\n",
    "  0.19337554822639866,\n",
    "  0.16536105377239416,\n",
    "  0.1571138994552154,\n",
    "  0.15273290199140158,\n",
    "  0.10815980746436514,\n",
    "  0.12133629483412643,\n",
    "  0.13833611768088946,\n",
    "  0.1293170387481046,\n",
    "  0.10733797376491747,\n",
    "  0.10590762594786797,\n",
    "  0.0918239034835805,\n",
    "  0.1047938061950121,\n",
    "  0.07058675783115197,\n",
    "  0.08766820735364987,\n",
    "  0.10277271190071632,\n",
    "  0.12322432884653176,\n",
    "  0.08019538524956828,\n",
    "  0.08205282129964776,\n",
    "  0.07426594978620334,\n",
    "  0.0755969595853548,\n",
    "  0.09760807308867492,\n",
    "  0.07207354542732404,\n",
    "  0.0975812500682325,\n",
    "  0.09537964315045605,\n",
    "  0.09366875433450493,\n",
    "  0.04696851966758542,\n",
    "  0.0512240380254569,\n",
    "  0.05459230239451631,\n",
    "  0.08089525441829507,\n",
    "  0.06131623744944018,\n",
    "  0.06199158364649636,\n",
    "  0.051481724839184166,\n",
    "  0.06538778840672245,\n",
    "  0.0629294374865063,\n",
    "  0.04425359480803349,\n",
    "  0.05855927577252546,\n",
    "  0.08699687327160525,\n",
    "  0.07487187519443134,\n",
    "  0.05951084620550851,\n",
    "  0.05211047360593793,\n",
    "  0.0654746382687632,\n",
    "  0.07681369263658207,\n",
    "  0.044812741151172154,\n",
    "  0.04169330514596971,\n",
    "  0.056397179180745924,\n",
    "  0.06540772032921424,\n",
    "  0.07633163423087057,\n",
    "  0.055633779746886776,\n",
    "  0.06838037114775641,\n",
    "  0.053590912112544256,\n",
    "  0.0628714390222539,\n",
    "  0.045393846311621903,\n",
    "  0.05751636609517408,\n",
    "  0.08074402875827821,\n",
    "  0.04321444615804029,\n",
    "  0.05079126861872594,\n",
    "  0.050103869856186006,\n",
    "  0.03507644276016325,\n",
    "  0.03688339269977289,\n",
    "  0.041359042481246576,\n",
    "  0.03544109247172077,\n",
    "  0.03485896200435596,\n",
    "  0.04210835051470699,\n",
    "  0.07559601816384749,\n",
    "  0.04357416177816813,\n",
    "  0.030860471100808837,\n",
    "  0.036541297235294604,\n",
    "  0.03869030817890365,\n",
    "  0.036192047033067894,\n",
    "  0.05795359878571838,\n",
    "  0.040659130261747875,\n",
    "  0.04481880535572273,\n",
    "  0.06331975865306445,\n",
    "  0.04256203787514518,\n",
    "  0.03762714751888046,\n",
    "  0.0230426168100116,\n",
    "  0.03029545205979716,\n",
    "  0.04663788862835142,\n",
    "  0.04618885129772497,\n",
    "  0.06090077068450866,\n",
    "  0.05378697729046914,\n",
    "  0.05114451131438682,\n",
    "  0.0467184322129791,\n",
    "  0.05389563671920387,\n",
    "  0.048165335642040105,\n",
    "  0.04513653215453111,\n",
    "  0.037053695992046956,\n",
    "  0.07355283905870348,\n",
    "  0.05021775581395428,\n",
    "  0.05387003534121017],\n",
    " [0.6489281767955801,\n",
    "  0.8121988950276243,\n",
    "  0.8585193370165746,\n",
    "  0.8681546961325967,\n",
    "  0.891889502762431,\n",
    "  0.9223425414364641,\n",
    "  0.906121546961326,\n",
    "  0.9344530386740332,\n",
    "  0.9319779005524862,\n",
    "  0.927646408839779,\n",
    "  0.925878453038674,\n",
    "  0.9390055248618785,\n",
    "  0.9413480662983426,\n",
    "  0.9398453038674033,\n",
    "  0.9611491712707182,\n",
    "  0.9558895027624309,\n",
    "  0.9517348066298342,\n",
    "  0.9534585635359116,\n",
    "  0.9620773480662983,\n",
    "  0.9622099447513812,\n",
    "  0.9653038674033149,\n",
    "  0.9622541436464088,\n",
    "  0.9747624309392265,\n",
    "  0.9688397790055249,\n",
    "  0.963403314917127,\n",
    "  0.9549613259668508,\n",
    "  0.9732154696132597,\n",
    "  0.9703867403314917,\n",
    "  0.974232044198895,\n",
    "  0.9735690607734807,\n",
    "  0.9656574585635359,\n",
    "  0.9750276243093923,\n",
    "  0.964817679558011,\n",
    "  0.9656132596685083,\n",
    "  0.9657016574585635,\n",
    "  0.9833370165745856,\n",
    "  0.9824530386740331,\n",
    "  0.9800220994475138,\n",
    "  0.9707845303867403,\n",
    "  0.9776795580110498,\n",
    "  0.9775027624309393,\n",
    "  0.9813480662983426,\n",
    "  0.9775911602209945,\n",
    "  0.9772375690607735,\n",
    "  0.9844861878453038,\n",
    "  0.9794475138121547,\n",
    "  0.9695469613259668,\n",
    "  0.9748066298342541,\n",
    "  0.9797569060773481,\n",
    "  0.981524861878453,\n",
    "  0.9777237569060774,\n",
    "  0.9734806629834254,\n",
    "  0.9838232044198895,\n",
    "  0.9848397790055249,\n",
    "  0.980243093922652,\n",
    "  0.9768839779005525,\n",
    "  0.9737016574585635,\n",
    "  0.9805966850828729,\n",
    "  0.9762651933701657,\n",
    "  0.9809060773480663,\n",
    "  0.978342541436464,\n",
    "  0.9840441988950276,\n",
    "  0.97953591160221,\n",
    "  0.9716685082872928,\n",
    "  0.9851491712707182,\n",
    "  0.9826298342541436,\n",
    "  0.9824972375690608,\n",
    "  0.9877127071823204,\n",
    "  0.9868729281767956,\n",
    "  0.9853259668508287,\n",
    "  0.9874475138121547,\n",
    "  0.9878453038674033,\n",
    "  0.9854143646408839,\n",
    "  0.9754696132596685,\n",
    "  0.9849723756906077,\n",
    "  0.9887734806629834,\n",
    "  0.9881546961325967,\n",
    "  0.9861215469613259,\n",
    "  0.9877569060773481,\n",
    "  0.9792707182320441,\n",
    "  0.9848397790055249,\n",
    "  0.984707182320442,\n",
    "  0.9776795580110498,\n",
    "  0.984707182320442,\n",
    "  0.9879779005524861,\n",
    "  0.9927513812154696,\n",
    "  0.9897458563535911,\n",
    "  0.9848839779005525,\n",
    "  0.9849281767955801,\n",
    "  0.9796685082872928,\n",
    "  0.9809502762430939,\n",
    "  0.9824530386740331,\n",
    "  0.9832044198895028,\n",
    "  0.9803314917127072,\n",
    "  0.9834254143646409,\n",
    "  0.9847513812154696,\n",
    "  0.9872265193370166,\n",
    "  0.9749834254143647,\n",
    "  0.9823204419889503,\n",
    "  0.982718232044199],\n",
    " [0.6211841620039762,\n",
    "  0.45876633668316125,\n",
    "  0.5034662859063835,\n",
    "  0.27011767895693073,\n",
    "  0.313655665158899,\n",
    "  0.20805899514211246,\n",
    "  0.3991670820106114,\n",
    "  0.32158903264995403,\n",
    "  0.24071570668090958,\n",
    "  0.2567666179594932,\n",
    "  0.3852716677423312,\n",
    "  0.4597050237921575,\n",
    "  0.17627169369967327,\n",
    "  0.23679502981565434,\n",
    "  0.17220754785561754,\n",
    "  0.18002146034804078,\n",
    "  0.22931813937735357,\n",
    "  0.16827500484832936,\n",
    "  0.15206738518692042,\n",
    "  0.14282209915193997,\n",
    "  0.18685480398813387,\n",
    "  0.11685570925779426,\n",
    "  0.20067864179665781,\n",
    "  0.1411727972821085,\n",
    "  0.1530777178196244,\n",
    "  0.13876111770846125,\n",
    "  0.14196548254774952,\n",
    "  0.2708777552003365,\n",
    "  0.11546770097299551,\n",
    "  0.3046354775860649,\n",
    "  0.10854558102802168,\n",
    "  0.07933161259128273,\n",
    "  0.12404691129011139,\n",
    "  0.38784327351626957,\n",
    "  0.07989218154476951,\n",
    "  0.17622217972792195,\n",
    "  0.09162371938435548,\n",
    "  0.13405118411540942,\n",
    "  0.10392435214123062,\n",
    "  0.10115272597895769,\n",
    "  0.09087322915753134,\n",
    "  0.1590506696334804,\n",
    "  0.22317246896386603,\n",
    "  0.08073770713501655,\n",
    "  0.058100116569523644,\n",
    "  0.14888879645115743,\n",
    "  0.13044291378562928,\n",
    "  0.07736094586014841,\n",
    "  0.11220023536174023,\n",
    "  0.13659915878864518,\n",
    "  0.07129768802977254,\n",
    "  0.20124599370539134,\n",
    "  0.07354985906765003,\n",
    "  0.08835384215030306,\n",
    "  0.21074753585173564,\n",
    "  0.09616019399096185,\n",
    "  0.11539241931760864,\n",
    "  0.120142063001071,\n",
    "  0.0775641988276421,\n",
    "  0.09695740938079764,\n",
    "  0.09058009887954965,\n",
    "  0.06536958760441548,\n",
    "  0.14220434166765117,\n",
    "  0.07930320302022702,\n",
    "  0.08943844097148232,\n",
    "  0.06493218757685769,\n",
    "  0.08265407143692353,\n",
    "  0.0510503365727251,\n",
    "  0.06409464690922057,\n",
    "  0.0902351198168243,\n",
    "  0.06698930089201968,\n",
    "  0.07941066838737426,\n",
    "  0.27309605946031246,\n",
    "  0.10141229745728485,\n",
    "  0.08438794725363463,\n",
    "  0.06564738088682306,\n",
    "  0.07934415308124455,\n",
    "  0.11631594676814944,\n",
    "  0.11903673065906457,\n",
    "  0.09058221255546416,\n",
    "  0.12191057274405673,\n",
    "  0.1233115003656721,\n",
    "  0.10492566676817347,\n",
    "  0.11040246008239304,\n",
    "  0.0784957536592911,\n",
    "  0.05796452499581156,\n",
    "  0.06605394684588234,\n",
    "  0.08701971364890956,\n",
    "  0.23499713032428876,\n",
    "  0.14679802382028134,\n",
    "  0.0931847726414798,\n",
    "  0.1281534519503502,\n",
    "  0.09721326736259316,\n",
    "  0.0960857172550666,\n",
    "  0.1704642465234064,\n",
    "  0.11685481813445657,\n",
    "  0.14966437045596195,\n",
    "  0.22490323309782892,\n",
    "  0.09092609374476246,\n",
    "  0.08504296933522287],\n",
    " [0.7440445139975657,\n",
    "  0.8125543383759346,\n",
    "  0.8101199791340636,\n",
    "  0.9012345679012346,\n",
    "  0.869414014953921,\n",
    "  0.9231438010780734,\n",
    "  0.8497652582159625,\n",
    "  0.892018779342723,\n",
    "  0.9128847157016171,\n",
    "  0.9021039819161885,\n",
    "  0.8753260302556077,\n",
    "  0.8575899843505478,\n",
    "  0.9285341679707877,\n",
    "  0.9189706138062945,\n",
    "  0.9384454877412624,\n",
    "  0.9398365501651886,\n",
    "  0.9163623717614328,\n",
    "  0.94679186228482,\n",
    "  0.9419231438010781,\n",
    "  0.942792557816032,\n",
    "  0.9297513475917232,\n",
    "  0.952529994783516,\n",
    "  0.9400104329681794,\n",
    "  0.9497478699356634,\n",
    "  0.9521822291775344,\n",
    "  0.9539210572074421,\n",
    "  0.9410537297861241,\n",
    "  0.9031472787341331,\n",
    "  0.9601808381151105,\n",
    "  0.8941053729786124,\n",
    "  0.9612241349330551,\n",
    "  0.9688749782646496,\n",
    "  0.9575725960702487,\n",
    "  0.8626325856372805,\n",
    "  0.9680055642496957,\n",
    "  0.9514866979655712,\n",
    "  0.9640062597809077,\n",
    "  0.952529994783516,\n",
    "  0.9575725960702487,\n",
    "  0.9641801425838985,\n",
    "  0.9650495565988524,\n",
    "  0.9384454877412624,\n",
    "  0.9297513475917232,\n",
    "  0.9683533298556772,\n",
    "  0.9746131107633456,\n",
    "  0.9436619718309859,\n",
    "  0.9544427056164145,\n",
    "  0.9688749782646496,\n",
    "  0.9613980177360459,\n",
    "  0.9547904712223961,\n",
    "  0.9721787515214745,\n",
    "  0.9337506520605112,\n",
    "  0.9754825247782994,\n",
    "  0.968527212658668,\n",
    "  0.9198400278212485,\n",
    "  0.9629629629629629,\n",
    "  0.9573987132672579,\n",
    "  0.9657450878108155,\n",
    "  0.97339593114241,\n",
    "  0.96452790818988,\n",
    "  0.9657450878108155,\n",
    "  0.9735698139454008,\n",
    "  0.9497478699356634,\n",
    "  0.9697443922796035,\n",
    "  0.9680055642496957,\n",
    "  0.9721787515214745,\n",
    "  0.970961571900539,\n",
    "  0.9780907668231612,\n",
    "  0.974960876369327,\n",
    "  0.9699182750825943,\n",
    "  0.9723526343244653,\n",
    "  0.9711354547035298,\n",
    "  0.9167101373674144,\n",
    "  0.9673100330377326,\n",
    "  0.9711354547035298,\n",
    "  0.9798295948530691,\n",
    "  0.9725265171274561,\n",
    "  0.9643540253868892,\n",
    "  0.9551382368283776,\n",
    "  0.9697443922796035,\n",
    "  0.9587897756911842,\n",
    "  0.9591375412971657,\n",
    "  0.970961571900539,\n",
    "  0.9648756737958616,\n",
    "  0.9720048687184838,\n",
    "  0.9810467744740046,\n",
    "  0.975830290384281,\n",
    "  0.9728742827334377,\n",
    "  0.9300991131977048,\n",
    "  0.9553121196313684,\n",
    "  0.9706138062945575,\n",
    "  0.9601808381151105,\n",
    "  0.9674839158407234,\n",
    "  0.9673100330377326,\n",
    "  0.9394887845592071,\n",
    "  0.9636584941749261,\n",
    "  0.9558337680403408,\n",
    "  0.9257520431229351,\n",
    "  0.9659189706138063,\n",
    "  0.970961571900539])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}